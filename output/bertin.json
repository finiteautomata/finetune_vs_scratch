{
    "context_hate": [
        {
            "eval_loss": 0.08282887935638428,
            "eval_calls_f1": 0.5980392156862745,
            "eval_women_f1": 0.4035714285714286,
            "eval_lgbti_f1": 0.46938775510204084,
            "eval_racism_f1": 0.6449760765550239,
            "eval_class_f1": 0.4507042253521127,
            "eval_politics_f1": 0.580327868852459,
            "eval_disabled_f1": 0.4584717607973422,
            "eval_appearance_f1": 0.6869671132764921,
            "eval_criminal_f1": 0.6072772898368882,
            "eval_mean_f1": 0.544413685798645,
            "eval_mean_precision": 0.5163614749908447,
            "eval_mean_recall": 0.5789512991905212,
            "eval_hate_precision": 0.6328600405679513,
            "eval_hate_recall": 0.6944908180300501,
            "eval_hate_f1": 0.6622446272220748,
            "eval_runtime": 35.912,
            "eval_samples_per_second": 315.855,
            "eval_steps_per_second": 19.743
        },
        {
            "eval_loss": 0.0820653885602951,
            "eval_calls_f1": 0.6029173419773095,
            "eval_women_f1": 0.42732049036777586,
            "eval_lgbti_f1": 0.4673366834170854,
            "eval_racism_f1": 0.6304347826086957,
            "eval_class_f1": 0.4444444444444444,
            "eval_politics_f1": 0.5979381443298969,
            "eval_disabled_f1": 0.44516129032258067,
            "eval_appearance_f1": 0.7054545454545454,
            "eval_criminal_f1": 0.6301020408163265,
            "eval_mean_f1": 0.5501232743263245,
            "eval_mean_precision": 0.515468180179596,
            "eval_mean_recall": 0.5928707122802734,
            "eval_hate_precision": 0.6408700050581689,
            "eval_hate_recall": 0.7050639955481358,
            "eval_hate_f1": 0.6714361420243773,
            "eval_runtime": 36.1112,
            "eval_samples_per_second": 314.113,
            "eval_steps_per_second": 19.634
        },
        {
            "eval_loss": 0.08691227436065674,
            "eval_calls_f1": 0.5913312693498453,
            "eval_women_f1": 0.4082332761578045,
            "eval_lgbti_f1": 0.45023696682464454,
            "eval_racism_f1": 0.6287262872628726,
            "eval_class_f1": 0.44745762711864406,
            "eval_politics_f1": 0.5728476821192052,
            "eval_disabled_f1": 0.4721311475409836,
            "eval_appearance_f1": 0.6982968369829684,
            "eval_criminal_f1": 0.5761006289308176,
            "eval_mean_f1": 0.5383735299110413,
            "eval_mean_precision": 0.49802365899086,
            "eval_mean_recall": 0.5887877941131592,
            "eval_hate_precision": 0.6135241855873642,
            "eval_hate_recall": 0.6917084028937117,
            "eval_hate_f1": 0.6502746534135495,
            "eval_runtime": 35.9074,
            "eval_samples_per_second": 315.896,
            "eval_steps_per_second": 19.745
        }
    ],
    "hate": [
        {
            "eval_loss": 0.6341549158096313,
            "eval_ok_f1": 0.7630385487528345,
            "eval_ok_precision": 0.816747572815534,
            "eval_ok_recall": 0.7159574468085106,
            "eval_hateful_f1": 0.7089136490250696,
            "eval_hateful_precision": 0.6559278350515464,
            "eval_hateful_recall": 0.7712121212121212,
            "eval_micro_f1": 0.73875,
            "eval_macro_f1": 0.7359760999679565,
            "eval_macro_precision": 0.7363376617431641,
            "eval_macro_recall": 0.7435847520828247,
            "eval_acc": 0.73875,
            "eval_runtime": 3.18,
            "eval_samples_per_second": 503.147,
            "eval_steps_per_second": 31.447,
            "epoch": 5.0
        },
        {
            "eval_loss": 0.7689306139945984,
            "eval_ok_f1": 0.7971175166297118,
            "eval_ok_precision": 0.8321759259259259,
            "eval_ok_recall": 0.7648936170212766,
            "eval_hateful_f1": 0.7378223495702007,
            "eval_hateful_precision": 0.6997282608695652,
            "eval_hateful_recall": 0.7803030303030303,
            "eval_micro_f1": 0.77125,
            "eval_macro_f1": 0.7674699425697327,
            "eval_macro_precision": 0.7659521102905273,
            "eval_macro_recall": 0.7725982666015625,
            "eval_acc": 0.77125,
            "eval_runtime": 3.2936,
            "eval_samples_per_second": 485.788,
            "eval_steps_per_second": 30.362,
            "epoch": 5.0
        },
        {
            "eval_loss": 0.7430853843688965,
            "eval_ok_f1": 0.7822626492325185,
            "eval_ok_precision": 0.8400488400488401,
            "eval_ok_recall": 0.7319148936170212,
            "eval_hateful_f1": 0.7342123525329631,
            "eval_hateful_precision": 0.677336747759283,
            "eval_hateful_recall": 0.8015151515151515,
            "eval_micro_f1": 0.760625,
            "eval_macro_f1": 0.7582374811172485,
            "eval_macro_precision": 0.7586928009986877,
            "eval_macro_recall": 0.7667150497436523,
            "eval_acc": 0.760625,
            "eval_runtime": 3.4678,
            "eval_samples_per_second": 461.382,
            "eval_steps_per_second": 28.836,
            "epoch": 5.0
        }
    ],
    "sentiment": [
        {
            "eval_loss": 0.7829428315162659,
            "eval_neg_f1": 0.7233966956830699,
            "eval_neg_precision": 0.7568773234200743,
            "eval_neg_recall": 0.692752636951344,
            "eval_neu_f1": 0.5320886814469078,
            "eval_neu_precision": 0.49159120310478654,
            "eval_neu_recall": 0.5798575788402849,
            "eval_pos_f1": 0.7403554399653229,
            "eval_pos_precision": 0.7574279379157428,
            "eval_pos_recall": 0.7240356083086054,
            "eval_micro_f1": 0.6723568281938326,
            "eval_macro_f1": 0.665280282497406,
            "eval_macro_precision": 0.6686322093009949,
            "eval_macro_recall": 0.6655486226081848,
            "eval_acc": 0.6723568281938326,
            "eval_runtime": 8.4067,
            "eval_samples_per_second": 864.076,
            "eval_steps_per_second": 27.002,
            "epoch": 5.0
        },
        {
            "eval_loss": 0.8057689070701599,
            "eval_neg_f1": 0.7411444141689374,
            "eval_neg_precision": 0.7419024889191954,
            "eval_neg_recall": 0.7403878870364069,
            "eval_neu_f1": 0.523680981595092,
            "eval_neu_precision": 0.5059269796111902,
            "eval_neu_recall": 0.5427263479145473,
            "eval_pos_f1": 0.7369569962890199,
            "eval_pos_precision": 0.7596759675967597,
            "eval_pos_recall": 0.7155574395930479,
            "eval_micro_f1": 0.6788270925110133,
            "eval_macro_f1": 0.6672608256340027,
            "eval_macro_precision": 0.6691684722900391,
            "eval_macro_recall": 0.6662238836288452,
            "eval_acc": 0.6788270925110133,
            "eval_runtime": 8.8879,
            "eval_samples_per_second": 817.294,
            "eval_steps_per_second": 25.54,
            "epoch": 5.0
        },
        {
            "eval_loss": 0.7679075002670288,
            "eval_neg_f1": 0.7346733668341708,
            "eval_neg_precision": 0.7235235895743979,
            "eval_neg_recall": 0.7461721674038788,
            "eval_neu_f1": 0.517433046993431,
            "eval_neu_precision": 0.5140562248995983,
            "eval_neu_recall": 0.5208545269582909,
            "eval_pos_f1": 0.7352173913043478,
            "eval_pos_precision": 0.7545738509593931,
            "eval_pos_recall": 0.7168291649003815,
            "eval_micro_f1": 0.6756607929515418,
            "eval_macro_f1": 0.6624412536621094,
            "eval_macro_precision": 0.6640512347221375,
            "eval_macro_recall": 0.6612852811813354,
            "eval_acc": 0.6756607929515418,
            "eval_runtime": 8.6263,
            "eval_samples_per_second": 842.081,
            "eval_steps_per_second": 26.315,
            "epoch": 5.0
        }
    ],
    "emotion": [
        {
            "eval_loss": 1.188679575920105,
            "eval_others_f1": 0.7491898898250163,
            "eval_others_precision": 0.8072625698324022,
            "eval_others_recall": 0.6989117291414753,
            "eval_joy_f1": 0.6753246753246753,
            "eval_joy_precision": 0.6372549019607843,
            "eval_joy_recall": 0.7182320441988951,
            "eval_sadness_f1": 0.7536945812807883,
            "eval_sadness_precision": 0.7391304347826086,
            "eval_sadness_recall": 0.7688442211055276,
            "eval_anger_f1": 0.5271317829457365,
            "eval_anger_precision": 0.4700460829493088,
            "eval_anger_recall": 0.6,
            "eval_surprise_f1": 0.43165467625899284,
            "eval_surprise_precision": 0.4225352112676056,
            "eval_surprise_recall": 0.4411764705882353,
            "eval_disgust_f1": 0.12121212121212122,
            "eval_disgust_precision": 0.12121212121212122,
            "eval_disgust_recall": 0.12121212121212122,
            "eval_fear_f1": 0.6046511627906976,
            "eval_fear_precision": 0.52,
            "eval_fear_recall": 0.7222222222222222,
            "eval_micro_f1": 0.6797853309481217,
            "eval_macro_f1": 0.5518369674682617,
            "eval_macro_precision": 0.5310630202293396,
            "eval_macro_recall": 0.5815140604972839,
            "eval_acc": 0.6797853309481217,
            "eval_runtime": 4.4086,
            "eval_samples_per_second": 380.39,
            "eval_steps_per_second": 23.817,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.1914745569229126,
            "eval_others_f1": 0.6997900629811057,
            "eval_others_precision": 0.8305647840531561,
            "eval_others_recall": 0.6045949214026602,
            "eval_joy_f1": 0.6758104738154614,
            "eval_joy_precision": 0.615909090909091,
            "eval_joy_recall": 0.7486187845303868,
            "eval_sadness_f1": 0.7446300715990454,
            "eval_sadness_precision": 0.7090909090909091,
            "eval_sadness_recall": 0.7839195979899497,
            "eval_anger_f1": 0.46997389033942555,
            "eval_anger_precision": 0.4225352112676056,
            "eval_anger_recall": 0.5294117647058824,
            "eval_surprise_f1": 0.34090909090909094,
            "eval_surprise_precision": 0.2777777777777778,
            "eval_surprise_recall": 0.4411764705882353,
            "eval_disgust_f1": 0.12658227848101267,
            "eval_disgust_precision": 0.10869565217391304,
            "eval_disgust_recall": 0.15151515151515152,
            "eval_fear_f1": 0.36363636363636365,
            "eval_fear_precision": 0.25,
            "eval_fear_recall": 0.6666666666666666,
            "eval_micro_f1": 0.6344663088849135,
            "eval_macro_f1": 0.4887617230415344,
            "eval_macro_precision": 0.45922476053237915,
            "eval_macro_recall": 0.5608433485031128,
            "eval_acc": 0.6344663088849135,
            "eval_runtime": 4.859,
            "eval_samples_per_second": 345.135,
            "eval_steps_per_second": 21.61,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.2430336475372314,
            "eval_others_f1": 0.7204884667571235,
            "eval_others_precision": 0.8207109737248841,
            "eval_others_recall": 0.6420798065296252,
            "eval_joy_f1": 0.6823821339950372,
            "eval_joy_precision": 0.6193693693693694,
            "eval_joy_recall": 0.7596685082872928,
            "eval_sadness_f1": 0.7458432304038005,
            "eval_sadness_precision": 0.7072072072072072,
            "eval_sadness_recall": 0.7889447236180904,
            "eval_anger_f1": 0.4736842105263158,
            "eval_anger_precision": 0.42857142857142855,
            "eval_anger_recall": 0.5294117647058824,
            "eval_surprise_f1": 0.3866666666666667,
            "eval_surprise_precision": 0.35365853658536583,
            "eval_surprise_recall": 0.4264705882352941,
            "eval_disgust_f1": 0.15584415584415584,
            "eval_disgust_precision": 0.13636363636363635,
            "eval_disgust_recall": 0.18181818181818182,
            "eval_fear_f1": 0.5652173913043479,
            "eval_fear_precision": 0.4642857142857143,
            "eval_fear_recall": 0.7222222222222222,
            "eval_micro_f1": 0.6565295169946332,
            "eval_macro_f1": 0.5328751802444458,
            "eval_macro_precision": 0.5043095350265503,
            "eval_macro_recall": 0.5786594152450562,
            "eval_acc": 0.6565295169946332,
            "eval_runtime": 4.6967,
            "eval_samples_per_second": 357.061,
            "eval_steps_per_second": 22.356,
            "epoch": 5.0
        }
    ],
    "model_name": "bertin-project/bertin-roberta-base-spanish"
}