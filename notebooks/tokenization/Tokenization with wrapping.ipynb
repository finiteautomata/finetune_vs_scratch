{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(tweet_files[0])])[:1_000_000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "len(tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "TemplateProcessing?"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mTemplateProcessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msingle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecial_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Provides a way to specify templates in order to add the special tokens to each\n",
      "input sequence as relevant.\n",
      "\n",
      "Let's take :obj:`BERT` tokenizer as an example. It uses two special tokens, used to\n",
      "delimitate each sequence. :obj:`[CLS]` is always used at the beginning of the first\n",
      "sequence, and :obj:`[SEP]` is added at the end of both the first, and the pair\n",
      "sequences. The final result looks like this:\n",
      "\n",
      "    - Single sequence: :obj:`[CLS] Hello there [SEP]`\n",
      "    - Pair sequences: :obj:`[CLS] My name is Anthony [SEP] What is my name? [SEP]`\n",
      "\n",
      "With the type ids as following::\n",
      "\n",
      "    [CLS]   ...   [SEP]   ...   [SEP]\n",
      "      0      0      0      1      1\n",
      "\n",
      "You can achieve such behavior using a TemplateProcessing::\n",
      "\n",
      "    TemplateProcessing(\n",
      "        single=\"[CLS] $0 [SEP]\",\n",
      "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
      "        special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 0)],\n",
      "    )\n",
      "\n",
      "In this example, each input sequence is identified using a ``$`` construct. This identifier\n",
      "lets us specify each input sequence, and the type_id to use. When nothing is specified,\n",
      "it uses the default values. Here are the different ways to specify it:\n",
      "\n",
      "    - Specifying the sequence, with default ``type_id == 0``: ``$A`` or ``$B``\n",
      "    - Specifying the `type_id` with default ``sequence == A``: ``$0``, ``$1``, ``$2``, ...\n",
      "    - Specifying both: ``$A:0``, ``$B:1``, ...\n",
      "\n",
      "The same construct is used for special tokens: ``<identifier>(:<type_id>)?``.\n",
      "\n",
      "**Warning**: You must ensure that you are giving the correct tokens/ids as these\n",
      "will be added to the Encoding without any further check. If the given ids correspond\n",
      "to something totally different in a `Tokenizer` using this `PostProcessor`, it\n",
      "might lead to unexpected results.\n",
      "\n",
      "Args:\n",
      "    single (:obj:`Template`):\n",
      "        The template used for single sequences\n",
      "\n",
      "    pair (:obj:`Template`):\n",
      "        The template used when both sequences are specified\n",
      "\n",
      "    special_tokens (:obj:`Tokens`):\n",
      "        The list of special tokens used in each sequences\n",
      "\n",
      "Types:\n",
      "\n",
      "    Template (:obj:`str` or :obj:`List`):\n",
      "        - If a :obj:`str` is provided, the whitespace is used as delimiter between tokens\n",
      "        - If a :obj:`List[str]` is provided, a list of tokens\n",
      "\n",
      "    Tokens (:obj:`List[Union[Tuple[int, str], Tuple[str, int], dict]]`):\n",
      "        - A :obj:`Tuple` with both a token and its associated ID, in any order\n",
      "        - A :obj:`dict` with the following keys:\n",
      "            - \"id\": :obj:`str` => The special token id, as specified in the Template\n",
      "            - \"ids\": :obj:`List[int]` => The associated IDs\n",
      "            - \"tokens\": :obj:`List[str]` => The associated tokens\n",
      "\n",
      "         The given dict expects the provided :obj:`ids` and :obj:`tokens` lists to have\n",
      "         the same length.\n",
      "\u001b[0;31mFile:\u001b[0m           ~/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages/tokenizers/processors/__init__.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers import normalizers \n",
    "from tokenizers.processors import RobertaProcessing\n",
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "#replacement=\"_\")\n",
    "\n",
    "strip_accents = True\n",
    "lowercase = True\n",
    "tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "\n",
    "tokenizer_normalizers = [\n",
    "    normalizers.NFKC(),\n",
    "    normalizers.BertNormalizer(\n",
    "        clean_text=True,\n",
    "        handle_chinese_chars=True,\n",
    "        strip_accents=strip_accents,\n",
    "        lowercase=lowercase,\n",
    "    )\n",
    "]\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(tokenizer_normalizers)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "tokenizer.post_processor = RobertaProcessing(\n",
    "    cls=(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "    sep=(\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "#tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=300,\n",
    "    special_tokens=tokenizer_special_tokens + special_tokens,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alphabet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "inv_vocab = {v:k for k, v in vocab.items()}\n",
    "inv_vocab = [inv_vocab[i] for i in range(len(inv_vocab))]\n",
    "\n",
    "print(f\"First tokens: {inv_vocab[:200]}\")\n",
    "\n",
    "alphabet = sorted(list({a for x in tokenizer.get_vocab() for a in x}))\n",
    "print(\"Alphabet = \", \" \".join(alphabet))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First tokens: ['<s>', '<pad>', '</s>', '<unk>', '<mask>', '@usuario', 'url', 'hashtag', 'emoji', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}', '¡', '¬', '¿', 'ı', 'ɪ', 'а', 'е', 'и', 'к', 'н', 'о', 'с', 'т', '،', 'ء', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ـ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ى', 'ي', 'ٹ', 'پ', 'چ', 'ڑ', 'ک', 'گ', 'ں', 'ھ', 'ہ', 'ۃ', 'ی', 'ے', '۔', 'क', 'ज', 'त', 'द', 'न', 'प', 'ब', 'म', 'य', 'र', 'ल', 'स', 'ह', 'ा', 'ि', 'ी', 'ो', 'ก', 'ค', 'ง', 'ด', 'น', 'ม', 'ย', 'ร', 'ล', 'ว', 'อ', 'า', 'เ', 'ᄀ', 'ᄁ', 'ᄂ', 'ᄃ', 'ᄄ', 'ᄅ', 'ᄆ', 'ᄇ', 'ᄉ', 'ᄊ', 'ᄋ', 'ᄌ', 'ᄍ', 'ᄎ', 'ᄏ', 'ᄐ', 'ᄑ', 'ᄒ', 'ᅡ', 'ᅢ', 'ᅣ', 'ᅥ', 'ᅦ', 'ᅧ', 'ᅨ', 'ᅩ', 'ᅪ', 'ᅬ', 'ᅭ', 'ᅮ', 'ᅯ', 'ᅱ', 'ᅲ', 'ᅳ', 'ᅴ', 'ᅵ', 'ᆨ', 'ᆫ']\n",
      "Alphabet =  ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ [ \\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z | } ¡ ¬ ¿ ı ɪ а е и к н о с т ، ء ا ب ة ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ـ ف ق ك ل م ن ه و ى ي ٹ پ چ ڑ ک گ ں ھ ہ ۃ ی ے ۔ क ज त द न प ब म य र ल स ह ा ि ी ो ก ค ง ด น ม ย ร ล ว อ า เ ᄀ ᄁ ᄂ ᄃ ᄄ ᄅ ᄆ ᄇ ᄉ ᄊ ᄋ ᄌ ᄍ ᄎ ᄏ ᄐ ᄑ ᄒ ᅡ ᅢ ᅣ ᅥ ᅦ ᅧ ᅨ ᅩ ᅪ ᅬ ᅭ ᅮ ᅯ ᅱ ᅲ ᅳ ᅴ ᅵ ᆨ ᆫ ᆯ ᆷ ᆸ ᆺ ᆻ ᆼ ᴀ ᴇ ᴏ ⃣ → ━ ┃ ┓ ▁ █ ▶ ► ☀ ★ ☆ ☇ ♪ ⚒ ⚘ ⚠ ⛏ ✔ ❣ ➡ ⠀ ⬇ 、 。 「 」 あ い う お か き く け こ さ し す た ち っ つ て と な に の は ま も ゃ よ ら り る れ を ん ア イ ク コ シ ス タ ッ テ ト ハ ヒ フ ラ リ ル レ ロ ン ・ 一 不 了 人 你 博 可 哈 啊 好 我 日 是 王 的 🌧 🎙 👁 📽 🕳 🗣\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "tokenizer.encode(\"@usuario son UNA MIERDA\", \"Viva Perón\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '@usuario',\n",
       " '▁son',\n",
       " '▁una',\n",
       " '▁mierda',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '▁viva',\n",
       " '▁peron',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    sep_token=\"</s>\",\n",
    "    cls_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "\n",
    "transformer_tokenizer.save_pretrained(\"small\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('small/tokenizer_config.json',\n",
       " 'small/special_tokens_map.json',\n",
       " 'small/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "from transformers import AutoTokenizer\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(\"small\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'bos_token': '<s>', 'eos_token': '</s>', 'sep_token': '</s>', 'cls_token': '<s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'special_tokens_map_file': 'small/special_tokens_map.json', 'tokenizer_file': 'small/tokenizer.json', 'name_or_path': 'small'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "transformer_tokenizer._tokenizer.encode(\"Este es un forro @usuario impresionánte\", \"Corte gil corte basura\").tokens\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '▁este',\n",
       " '▁es',\n",
       " '▁un',\n",
       " '▁forro',\n",
       " '▁',\n",
       " '@usuario',\n",
       " '▁impresionante',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '▁corte',\n",
       " '▁gil',\n",
       " '▁corte',\n",
       " '▁basura',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'model_max_length': 512, 'vocab_file': '/home/jmperez/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab', 'merges_file': '/home/jmperez/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b', 'tokenizer_file': '/home/jmperez/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730', 'special_tokens_map_file': None, 'name_or_path': 'roberta-base'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "\n",
    "tokenizer._tokenizer.encode(\"Oh man this is terrible\", \"Bullshit\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Oh',\n",
       " 'Ġman',\n",
       " 'Ġthis',\n",
       " 'Ġis',\n",
       " 'Ġterrible',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'Bull',\n",
       " 'shit',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}