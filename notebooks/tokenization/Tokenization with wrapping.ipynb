{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(tweet_files[0])])[:1_000_000]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "source": [
    "len(tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "\n",
    "SentencePieceBPETokenizer().normalizer"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tokenizers.normalizers.NFKC at 0x7f792b1e6770>"
      ]
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "from tokenizers import SentencePieceUnigramTokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers import normalizers \n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "#replacement=\"_\")\n",
    "\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence([\n",
    "    normalizers.Lowercase(),\n",
    "    normalizers.NFKC()\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "#tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=300,\n",
    "    special_tokens=tokenizer_special_tokens + special_tokens,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alphabet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'! \" # $ % & \\' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A E I N O S [ \\\\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ¡ ¬ ¿ à á â ã ä ç è é ê ì í î ï ñ ò ó ô õ ö ù ú ü ć č đ ğ ı ş š ž ɪ ɴ ˈ ̄ ̇ ̶ а в д е и к л м н о р с т у ، ؟ ء آ أ ؤ إ ئ ا ب ة ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ـ ف ق ك ل م ن ه و ى ي ً ٍ َ ُ ِ ّ ْ ٰ ٹ پ چ ڑ ک گ ں ھ ہ ۃ ی ے ۔ \\u06dd ۡ ं क ग ज त द न प ब म य र ल स ह ़ ा ि ी ु े ै ो ् ก ค ง ด ต ท น บ ม ย ร ล ว ห อ ั า ี เ แ ่ ้ ᄏ ᅲ ᴀ ᴇ ᴏ \\u200b \\u200d \\u2060 \\u2063 \\u2066 \\u2069 ⃢ ⃣ → ━ ┃ ┓ ┗ ┛ ▁ █ ▪ ▶ ► ☀ ★ ☆ ☇ ☑ ♪ ♫ ⚒ ⚘ ⚠ ⛏ ✍ ✔ ❣ ➡ ➽ ⠀ ⬇ 、 。 「 」 あ い う お か が き く け こ さ し す た だ ち っ つ て で と な に の は ま も ゃ よ ら り る れ を ん ア イ ォ ク コ シ ジ ス タ チ ッ ツ ト ニ ノ バ フ プ マ ャ ラ リ ル レ ロ ン ・ 一 不 么 了 人 你 博 可 哈 啊 大 好 我 描 日 是 最 有 本 爱 王 的 真 가 거 게 결 고 국 그 근 기 김 나 남 내 너 년 는 늘 니 다 단 대 더 데 도 동 두 드 든 들 라 랑 럽 레 로 를 리 마 만 모 몬 무 미 민 박 방 백 번 보 불 븐 비 사 상 생 서 석 성 세 소 수 스 시 아 안 야 어 에 엑 엔 여 열 영 예 오 요 우 운 워 원 유 으 은 을 음 의 이 인 일 있 자 작 정 제 주 준 중 즈 지 직 진 차 찬 축 카 케 크 클 키 타 탄 태 투 트 티 팬 포 피 하 한 함 해 현 호 홉 화 훈 🌧 🍽 🎙 🏖 🏟 👁 📽 🕳 🗓 🗞 🗣'"
      ]
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "source": [
    "tokenizer.encode(\"@usuario son UNA MIERDA\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁sonr', 'den', '▁claro', '▁arri']"
      ]
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'@usuario tugo bierno skere comunista'"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "transformer_tokenizer.save_pretrained"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "normalizer = normalizers.BertNormalizer(clean_text=True, strip_accents=True)\n",
    "\n",
    "normalizer.normalize_str(\"álaba\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'alaba'"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}