{
    "hate": [
        {
            "eval_loss": 0.483341246843338,
            "eval_ok_f1": 0.821875,
            "eval_ok_precision": 0.8795986622073578,
            "eval_ok_recall": 0.7712609970674487,
            "eval_hateful_f1": 0.6833333333333333,
            "eval_hateful_precision": 0.6119402985074627,
            "eval_hateful_recall": 0.7735849056603774,
            "eval_micro_f1": 0.772,
            "eval_macro_f1": 0.7526041269302368,
            "eval_macro_precision": 0.7457695007324219,
            "eval_macro_recall": 0.7724229097366333,
            "eval_acc": 0.772,
            "eval_runtime": 0.6913,
            "eval_samples_per_second": 723.274,
            "eval_steps_per_second": 46.29,
            "epoch": 2.0
        },
        {
            "eval_loss": 0.5174441337585449,
            "eval_ok_f1": 0.8148148148148148,
            "eval_ok_precision": 0.8599348534201955,
            "eval_ok_recall": 0.7741935483870968,
            "eval_hateful_f1": 0.6590909090909091,
            "eval_hateful_precision": 0.6010362694300518,
            "eval_hateful_recall": 0.7295597484276729,
            "eval_micro_f1": 0.76,
            "eval_macro_f1": 0.7369529008865356,
            "eval_macro_precision": 0.7304855585098267,
            "eval_macro_recall": 0.7518765926361084,
            "eval_acc": 0.76,
            "eval_runtime": 0.6936,
            "eval_samples_per_second": 720.899,
            "eval_steps_per_second": 46.138,
            "epoch": 2.0
        }
    ],
    "context_hate": [
        {
            "eval_loss": 0.3462304174900055,
            "eval_calls_f1": 0.0,
            "eval_women_f1": 0.0,
            "eval_lgbti_f1": 0.0,
            "eval_racism_f1": 0.0,
            "eval_class_f1": 0.0,
            "eval_politics_f1": 0.0,
            "eval_disabled_f1": 0.0,
            "eval_appearance_f1": 0.0,
            "eval_criminal_f1": 0.0,
            "eval_mean_f1": 0.0,
            "eval_mean_precision": 0.0,
            "eval_mean_recall": 0.0,
            "eval_hate_precision": 0.0,
            "eval_hate_recall": 0.0,
            "eval_hate_f1": 0.0,
            "eval_runtime": 1.4715,
            "eval_samples_per_second": 339.789,
            "eval_steps_per_second": 21.746
        },
        {
            "eval_loss": 0.3700132369995117,
            "eval_calls_f1": 0.0,
            "eval_women_f1": 0.0,
            "eval_lgbti_f1": 0.0,
            "eval_racism_f1": 0.0,
            "eval_class_f1": 0.0,
            "eval_politics_f1": 0.0,
            "eval_disabled_f1": 0.0,
            "eval_appearance_f1": 0.0,
            "eval_criminal_f1": 0.0,
            "eval_mean_f1": 0.0,
            "eval_mean_precision": 0.0,
            "eval_mean_recall": 0.0,
            "eval_hate_precision": 0.0,
            "eval_hate_recall": 0.0,
            "eval_hate_f1": 0.0,
            "eval_runtime": 1.4729,
            "eval_samples_per_second": 339.459,
            "eval_steps_per_second": 21.725
        }
    ],
    "sentiment": [
        {
            "eval_loss": 0.9389091730117798,
            "eval_neg_f1": 0.6746031746031745,
            "eval_neg_precision": 0.5448717948717948,
            "eval_neg_recall": 0.8854166666666666,
            "eval_neu_f1": 0.12290502793296089,
            "eval_neu_precision": 0.6470588235294118,
            "eval_neu_recall": 0.06790123456790123,
            "eval_pos_f1": 0.6309148264984228,
            "eval_pos_precision": 0.5847953216374269,
            "eval_pos_recall": 0.684931506849315,
            "eval_micro_f1": 0.562,
            "eval_macro_f1": 0.47614097595214844,
            "eval_macro_precision": 0.5922420024871826,
            "eval_macro_recall": 0.5460831522941589,
            "eval_acc": 0.562,
            "eval_runtime": 0.6134,
            "eval_samples_per_second": 815.153,
            "eval_steps_per_second": 52.17,
            "epoch": 2.0
        },
        {
            "eval_loss": 0.9540478587150574,
            "eval_neg_f1": 0.6614173228346456,
            "eval_neg_precision": 0.5316455696202531,
            "eval_neg_recall": 0.875,
            "eval_neu_f1": 0.024242424242424242,
            "eval_neu_precision": 0.6666666666666666,
            "eval_neu_recall": 0.012345679012345678,
            "eval_pos_f1": 0.6055045871559633,
            "eval_pos_precision": 0.5469613259668509,
            "eval_pos_recall": 0.678082191780822,
            "eval_micro_f1": 0.538,
            "eval_macro_f1": 0.43038809299468994,
            "eval_macro_precision": 0.5817578434944153,
            "eval_macro_recall": 0.5218092799186707,
            "eval_acc": 0.538,
            "eval_runtime": 0.6121,
            "eval_samples_per_second": 816.878,
            "eval_steps_per_second": 52.28,
            "epoch": 2.0
        }
    ],
    "emotion": [
        {
            "eval_loss": 1.9113014936447144,
            "eval_others_f1": 0.4932975871313673,
            "eval_others_precision": 0.5822784810126582,
            "eval_others_recall": 0.42790697674418604,
            "eval_joy_f1": 0.36363636363636365,
            "eval_joy_precision": 0.5909090909090909,
            "eval_joy_recall": 0.26262626262626265,
            "eval_sadness_f1": 0.451063829787234,
            "eval_sadness_precision": 0.3464052287581699,
            "eval_sadness_recall": 0.6463414634146342,
            "eval_anger_f1": 0.21951219512195122,
            "eval_anger_precision": 0.32142857142857145,
            "eval_anger_recall": 0.16666666666666666,
            "eval_surprise_f1": 0.09523809523809523,
            "eval_surprise_precision": 0.2,
            "eval_surprise_recall": 0.0625,
            "eval_disgust_f1": 0.017241379310344827,
            "eval_disgust_precision": 0.009345794392523364,
            "eval_disgust_recall": 0.1111111111111111,
            "eval_fear_f1": 0.0,
            "eval_fear_precision": 0.0,
            "eval_fear_recall": 0.0,
            "eval_micro_f1": 0.366,
            "eval_macro_f1": 0.2342842072248459,
            "eval_macro_precision": 0.2929095923900604,
            "eval_macro_recall": 0.23959319293498993,
            "eval_acc": 0.366,
            "eval_runtime": 1.0825,
            "eval_samples_per_second": 461.9,
            "eval_steps_per_second": 29.562,
            "epoch": 2.0
        },
        {
            "eval_loss": 1.7949028015136719,
            "eval_others_f1": 0.2857142857142857,
            "eval_others_precision": 0.7450980392156863,
            "eval_others_recall": 0.17674418604651163,
            "eval_joy_f1": 0.44571428571428573,
            "eval_joy_precision": 0.5131578947368421,
            "eval_joy_recall": 0.3939393939393939,
            "eval_sadness_f1": 0.6224489795918368,
            "eval_sadness_precision": 0.5350877192982456,
            "eval_sadness_recall": 0.7439024390243902,
            "eval_anger_f1": 0.3769633507853403,
            "eval_anger_precision": 0.26277372262773724,
            "eval_anger_recall": 0.6666666666666666,
            "eval_surprise_f1": 0.21739130434782608,
            "eval_surprise_precision": 0.16666666666666666,
            "eval_surprise_recall": 0.3125,
            "eval_disgust_f1": 0.028169014084507043,
            "eval_disgust_precision": 0.016129032258064516,
            "eval_disgust_recall": 0.1111111111111111,
            "eval_fear_f1": 0.0,
            "eval_fear_precision": 0.0,
            "eval_fear_recall": 0.0,
            "eval_micro_f1": 0.37,
            "eval_macro_f1": 0.2823430597782135,
            "eval_macro_precision": 0.31984472274780273,
            "eval_macro_recall": 0.34355196356773376,
            "eval_acc": 0.37,
            "eval_runtime": 1.0817,
            "eval_samples_per_second": 462.219,
            "eval_steps_per_second": 29.582,
            "epoch": 2.0
        }
    ],
    "model_name": "dccuchile/bert-base-spanish-wwm-uncased"
}