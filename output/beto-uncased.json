{
    "context_hate": [
        {
            "eval_loss": 0.07744159549474716,
            "eval_calls_f1": 0.6690777576853526,
            "eval_women_f1": 0.3877995642701525,
            "eval_lgbti_f1": 0.48214285714285715,
            "eval_racism_f1": 0.685128205128205,
            "eval_class_f1": 0.5454545454545455,
            "eval_politics_f1": 0.5916955017301039,
            "eval_disabled_f1": 0.5745454545454546,
            "eval_appearance_f1": 0.7275031685678073,
            "eval_criminal_f1": 0.6234567901234568,
            "eval_mean_f1": 0.5874226689338684,
            "eval_mean_precision": 0.6216743588447571,
            "eval_mean_recall": 0.5643776655197144,
            "eval_hate_precision": 0.708308065494239,
            "eval_hate_recall": 0.6499721758486366,
            "eval_hate_f1": 0.677887405687754,
            "eval_runtime": 89.9402,
            "eval_samples_per_second": 126.117,
            "eval_steps_per_second": 7.883
        },
        {
            "eval_loss": 0.07789183408021927,
            "eval_calls_f1": 0.6762589928057553,
            "eval_women_f1": 0.4095860566448802,
            "eval_lgbti_f1": 0.47305389221556887,
            "eval_racism_f1": 0.6810699588477366,
            "eval_class_f1": 0.49816849816849806,
            "eval_politics_f1": 0.6276595744680851,
            "eval_disabled_f1": 0.6181818181818182,
            "eval_appearance_f1": 0.7516600265604249,
            "eval_criminal_f1": 0.6299694189602447,
            "eval_mean_f1": 0.5961787104606628,
            "eval_mean_precision": 0.6318784952163696,
            "eval_mean_recall": 0.5703458786010742,
            "eval_hate_precision": 0.7292954264524104,
            "eval_hate_recall": 0.6566499721758486,
            "eval_hate_f1": 0.6910688140556369,
            "eval_runtime": 90.1108,
            "eval_samples_per_second": 125.878,
            "eval_steps_per_second": 7.868
        },
        {
            "eval_loss": 0.07837081700563431,
            "eval_calls_f1": 0.6666666666666666,
            "eval_women_f1": 0.428,
            "eval_lgbti_f1": 0.4846796657381616,
            "eval_racism_f1": 0.6810598626104024,
            "eval_class_f1": 0.5070422535211268,
            "eval_politics_f1": 0.634315424610052,
            "eval_disabled_f1": 0.5338078291814947,
            "eval_appearance_f1": 0.7222946544980443,
            "eval_criminal_f1": 0.6358381502890174,
            "eval_mean_f1": 0.5881893634796143,
            "eval_mean_precision": 0.595319926738739,
            "eval_mean_recall": 0.5846154689788818,
            "eval_hate_precision": 0.6940509915014165,
            "eval_hate_recall": 0.6816917084028937,
            "eval_hate_f1": 0.6878158338012352,
            "eval_runtime": 90.1145,
            "eval_samples_per_second": 125.873,
            "eval_steps_per_second": 7.868
        }
    ],
    "sentiment": [
        {
            "eval_loss": 1.6634529829025269,
            "eval_neg_f1": 0.7263507606224865,
            "eval_neg_precision": 0.7471223021582734,
            "eval_neg_recall": 0.706702960190541,
            "eval_neu_f1": 0.5132535772929862,
            "eval_neu_precision": 0.4762734000870701,
            "eval_neu_recall": 0.5564598168870803,
            "eval_pos_f1": 0.7149142102947647,
            "eval_pos_precision": 0.7430269775948788,
            "eval_pos_recall": 0.688851208139042,
            "eval_micro_f1": 0.6602422907488987,
            "eval_macro_f1": 0.6515061855316162,
            "eval_macro_precision": 0.6554742455482483,
            "eval_macro_recall": 0.6506713628768921,
            "eval_acc": 0.6602422907488987,
            "eval_runtime": 27.3496,
            "eval_samples_per_second": 265.598,
            "eval_steps_per_second": 8.3,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.6459299325942993,
            "eval_neg_f1": 0.7199564980967917,
            "eval_neg_precision": 0.7703646237393328,
            "eval_neg_recall": 0.6757400476352501,
            "eval_neu_f1": 0.5226309921962096,
            "eval_neu_precision": 0.46526399364827314,
            "eval_neu_recall": 0.5961342828077314,
            "eval_pos_f1": 0.7185152452496686,
            "eval_pos_precision": 0.7503461005999077,
            "eval_pos_recall": 0.6892751165748199,
            "eval_micro_f1": 0.6585903083700441,
            "eval_macro_f1": 0.6537008881568909,
            "eval_macro_precision": 0.6619915962219238,
            "eval_macro_recall": 0.653716504573822,
            "eval_acc": 0.6585903083700441,
            "eval_runtime": 27.3564,
            "eval_samples_per_second": 265.532,
            "eval_steps_per_second": 8.298,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.6320914030075073,
            "eval_neg_f1": 0.7175212900887842,
            "eval_neg_precision": 0.7674418604651163,
            "eval_neg_recall": 0.6736985369173188,
            "eval_neu_f1": 0.5229040622299049,
            "eval_neu_precision": 0.45454545454545453,
            "eval_neu_recall": 0.6154628687690743,
            "eval_pos_f1": 0.7089705546678842,
            "eval_pos_precision": 0.768051434223541,
            "eval_pos_recall": 0.6583298007630352,
            "eval_micro_f1": 0.6529460352422908,
            "eval_macro_f1": 0.6497986316680908,
            "eval_macro_precision": 0.6633462905883789,
            "eval_macro_recall": 0.6491637229919434,
            "eval_acc": 0.6529460352422908,
            "eval_runtime": 27.3511,
            "eval_samples_per_second": 265.584,
            "eval_steps_per_second": 8.299,
            "epoch": 5.0
        }
    ],
    "emotion": [
        {
            "eval_loss": 1.6966780424118042,
            "eval_others_f1": 0.7459324155193993,
            "eval_others_precision": 0.7730220492866408,
            "eval_others_recall": 0.720677146311971,
            "eval_joy_f1": 0.6415584415584416,
            "eval_joy_precision": 0.6053921568627451,
            "eval_joy_recall": 0.6823204419889503,
            "eval_sadness_f1": 0.7308641975308643,
            "eval_sadness_precision": 0.7184466019417476,
            "eval_sadness_recall": 0.7437185929648241,
            "eval_anger_f1": 0.5920000000000001,
            "eval_anger_precision": 0.5414634146341464,
            "eval_anger_recall": 0.6529411764705882,
            "eval_surprise_f1": 0.3278688524590164,
            "eval_surprise_precision": 0.37037037037037035,
            "eval_surprise_recall": 0.29411764705882354,
            "eval_disgust_f1": 0.09090909090909091,
            "eval_disgust_precision": 0.18181818181818182,
            "eval_disgust_recall": 0.06060606060606061,
            "eval_fear_f1": 0.65,
            "eval_fear_precision": 0.5909090909090909,
            "eval_fear_recall": 0.7222222222222222,
            "eval_micro_f1": 0.6779964221824687,
            "eval_macro_f1": 0.5398761630058289,
            "eval_macro_precision": 0.5402030944824219,
            "eval_macro_recall": 0.5538004636764526,
            "eval_acc": 0.6779964221824687,
            "eval_runtime": 6.5501,
            "eval_samples_per_second": 256.027,
            "eval_steps_per_second": 16.03,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.7208929061889648,
            "eval_others_f1": 0.739760554505356,
            "eval_others_precision": 0.7723684210526316,
            "eval_others_recall": 0.7097944377267231,
            "eval_joy_f1": 0.6658097686375322,
            "eval_joy_precision": 0.6225961538461539,
            "eval_joy_recall": 0.7154696132596685,
            "eval_sadness_f1": 0.7582697201017812,
            "eval_sadness_precision": 0.7680412371134021,
            "eval_sadness_recall": 0.7487437185929648,
            "eval_anger_f1": 0.5925925925925926,
            "eval_anger_precision": 0.5384615384615384,
            "eval_anger_recall": 0.6588235294117647,
            "eval_surprise_f1": 0.33898305084745767,
            "eval_surprise_precision": 0.4,
            "eval_surprise_recall": 0.29411764705882354,
            "eval_disgust_f1": 0.11764705882352942,
            "eval_disgust_precision": 0.16666666666666666,
            "eval_disgust_recall": 0.09090909090909091,
            "eval_fear_f1": 0.4897959183673469,
            "eval_fear_precision": 0.3870967741935484,
            "eval_fear_recall": 0.6666666666666666,
            "eval_micro_f1": 0.6809779367918902,
            "eval_macro_f1": 0.5289798378944397,
            "eval_macro_precision": 0.5221758484840393,
            "eval_macro_recall": 0.5549321174621582,
            "eval_acc": 0.6809779367918902,
            "eval_runtime": 6.4933,
            "eval_samples_per_second": 258.265,
            "eval_steps_per_second": 16.17,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.3173816204071045,
            "eval_others_f1": 0.6984993178717598,
            "eval_others_precision": 0.8012519561815337,
            "eval_others_recall": 0.619105199516324,
            "eval_joy_f1": 0.6585662211421629,
            "eval_joy_precision": 0.5878524945770065,
            "eval_joy_recall": 0.7486187845303868,
            "eval_sadness_f1": 0.7392405063291139,
            "eval_sadness_precision": 0.7448979591836735,
            "eval_sadness_recall": 0.7336683417085427,
            "eval_anger_f1": 0.5227272727272728,
            "eval_anger_precision": 0.42592592592592593,
            "eval_anger_recall": 0.6764705882352942,
            "eval_surprise_f1": 0.3571428571428572,
            "eval_surprise_precision": 0.3472222222222222,
            "eval_surprise_recall": 0.36764705882352944,
            "eval_disgust_f1": 0.045454545454545456,
            "eval_disgust_precision": 0.09090909090909091,
            "eval_disgust_recall": 0.030303030303030304,
            "eval_fear_f1": 0.5217391304347826,
            "eval_fear_precision": 0.42857142857142855,
            "eval_fear_recall": 0.6666666666666666,
            "eval_micro_f1": 0.6451997614788313,
            "eval_macro_f1": 0.5061957240104675,
            "eval_macro_precision": 0.4895187318325043,
            "eval_macro_recall": 0.5489256978034973,
            "eval_acc": 0.6451997614788313,
            "eval_runtime": 6.4911,
            "eval_samples_per_second": 258.354,
            "eval_steps_per_second": 16.176,
            "epoch": 5.0
        }
    ],
    "model_name": "dccuchile/bert-base-spanish-wwm-uncased"
}