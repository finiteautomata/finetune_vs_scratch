{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(tweet_files[0])])[:100_000]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "len(tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "from tokenizers import SentencePieceUnigramTokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()#replacement=\"_\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "#tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=500,\n",
    "    special_tokens=tokenizer_special_tokens,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "inv_vocab = {v:k for k, v in vocab.items()}\n",
    "\n",
    "\n",
    "tokenizer.encode(\"Qué hacesssss @usuario\").tokens\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁Qué', '▁haces', 'ss', 'ss', '▁@usuario']"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "tokenizer_path = \"./sentence-piece-tokenizer\"\n",
    "!mkdir $tokenizer_path\n",
    "vocab_file, merges_file = tokenizer.save_model(tokenizer_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sentencepiece\n",
    "\n",
    "sentence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "\n",
    "from finetune_vs_scratch.tokenizer import MyTokenizer\n",
    "\n",
    "t_tokenizer = MyTokenizer(\n",
    "    vocab_file,\n",
    "    merges_file,\n",
    ")\n",
    "\n",
    "t_tokenizer\n",
    "#sorted(vars(t_tokenizer).keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='', vocab_size=30000, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'})"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "\n",
    "t_tokenizer(\"@usuario tugo bierno skere comunista\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [0, 534, 764, 624, 569, 1206, 13253, 926, 13757, 1], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from transformers import RobertaTokenizerFast, AutoTokenizer\n",
    "\n",
    "roberta_tokenizer = RobertaTokenizerFast(vocab_file, merges_file, never_split=special_tokens)\n",
    "bertweet_tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "(roberta_tokenizer(\"@usuario tugo bierno skere comunista\")[\"input_ids\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'vocab_file': '/home/jmperez/.cache/huggingface/transformers/9a877d0d57efbfeae96fec396a35595dc8c4685fe2b7b2049c6c094e24a0e8bf.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982', 'merges_file': '/home/jmperez/.cache/huggingface/transformers/1c2d05a06ac61a063ad62a7590731a28cc62f58e2802c76b5f993165f25894a9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4', 'added_tokens_file': None, 'special_tokens_map_file': None, 'tokenizer_config_file': None, 'tokenizer_file': None}\n",
      "() {'model_max_length': 128, 'vocab_file': '/home/jmperez/.cache/huggingface/transformers/9a877d0d57efbfeae96fec396a35595dc8c4685fe2b7b2049c6c094e24a0e8bf.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982', 'merges_file': '/home/jmperez/.cache/huggingface/transformers/1c2d05a06ac61a063ad62a7590731a28cc62f58e2802c76b5f993165f25894a9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4', 'special_tokens_map_file': None, 'tokenizer_file': None, 'name_or_path': 'vinai/bertweet-base'}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0, 34, 529, 653, 624, 68, 1206, 85, 77, 926, 7519, 919, 1]"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "inv_vocab = {v:k for k, v in roberta_tokenizer.vocab.items()}\n",
    "tok_ids = roberta_tokenizer(\"@usuario tugo bierno skere comunista\")[\"input_ids\"]\n",
    "\n",
    "for tok in tok_ids:\n",
    "    print(tok, \" ---> \", inv_vocab[tok])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0  --->  <s>\n",
      "34  --->  @\n",
      "529  --->  usuario\n",
      "653  --->  tu\n",
      "624  --->  go\n",
      "68  --->  b\n",
      "1206  --->  ierno\n",
      "85  --->  s\n",
      "77  --->  k\n",
      "926  --->  ere\n",
      "7519  --->  comun\n",
      "919  --->  ista\n",
      "1  --->  </s>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "inv_vocab = {v:k for k, v in t_tokenizer.encoder.items()}\n",
    "tok_ids = t_tokenizer(\"@usuario tugo bierno skere comunista\")[\"input_ids\"]\n",
    "\n",
    "for tok in tok_ids:\n",
    "    print(tok, \" ---> \", inv_vocab[tok])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0  --->  <s>\n",
      "534  --->  ▁@usuario\n",
      "764  --->  ▁tu\n",
      "624  --->  go\n",
      "569  --->  ▁b\n",
      "1206  --->  ierno\n",
      "13253  --->  ▁sk\n",
      "926  --->  ere\n",
      "13757  --->  ▁comunista\n",
      "1  --->  </s>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "%%timeit\n",
    "\n",
    "t_tokenizer(tweets[:1000]);\n",
    "\n",
    "None"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "501 ms ± 7.19 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "%%timeit\n",
    "\n",
    "roberta_tokenizer(tweets[:1000]);\n",
    "\n",
    "None"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "24.9 ms ± 1.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "%%timeit\n",
    "\n",
    "tokenizer.encode_batch(tweets[:1000])\n",
    "\n",
    "None"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18.1 ms ± 613 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "%%timeit\n",
    "\n",
    "bertweet_tokenizer(tweets[:1000])\n",
    "\n",
    "None"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (129 > 128). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "103 ms ± 10.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La implementación nuestra es muy muy mala"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}