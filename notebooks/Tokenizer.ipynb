{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(\"../data/filtered_tweets/spanish-tweets-000.txt\")])[:1_000_000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "len(tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "5224220"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filtered_batch = filter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False,\n",
    "    lowercase=False,\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=40_000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + special_tokens,\n",
    "    limit_alphabet=600,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!mkdir test_tokenizer\n",
    "tokenizer.save_model(\"./test_tokenizer/\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘test_tokenizer’: File exists\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./test_tokenizer/vocab.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reload"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"./test_tokenizer/\",\n",
    "    never_split=special_tokens,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading trained tokenizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\n",
    "!pip freeze | grep transf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: Could not generate requirement for distribution -orch 1.9.0+cu111 (/home/jmperez/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages): Parse error at \"'-orch==1'\": Expected W:(abcd...)\u001b[0m\n",
      "transformers @ file:///home/jmperez/.cache/pypoetry/artifacts/d7/d1/6f/c08a86d09ebb99ef7233126f12dce131f0bcf38f23b1c8aa8d2065c528/transformers-4.8.2-py3-none-any.whl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer, BertTokenizer\n",
    "from finetune_vs_scratch.model import load_tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"../models/tokenizers/betito_cased_accents/\",\n",
    "    never_split=[\"@usuario\"]\n",
    ")\n",
    "\n",
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Si es Fast, le tenemos que poner `additional_special_tokens`, LTA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer, BertTokenizer\n",
    "from finetune_vs_scratch.model import load_tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"../models/tokenizers/betito_cased_accents/\",\n",
    "    additional_special_tokens=[\"@usuario\"]\n",
    ")\n",
    "\n",
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 38, 1164, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import re \n",
    "\n",
    "tweet = \"@usuario @usuario jajaja gil emoji riendo emoji emoji riendo emoji\"\n",
    "\n",
    "tweet = re.sub(\"emoji.*?emoji\", \"emoji\", tweet)\n",
    "print(tweet)\n",
    "tokens = bert_tokenizer(tweet)[\"input_ids\"]\n",
    "print(tokens)\n",
    "print(bert_tokenizer.decode(tokens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "@usuario @usuario jajaja gil emoji emoji\n",
      "[2, 38, 1164, 38, 1164, 5733, 621, 5411, 8, 8, 3]\n",
      "[CLS] @ usuario @ usuario jajaja gil emoji emoji [SEP]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=False, do_lower_case=True, do_basic_tokenize=True)\n",
    "tokenizer.add_tokens(['graft', 'grafts'])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4bd9500643240529c87f4d39be57585"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e43bf239ef5145f1a3a49fba534210bc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a902248c7b547af94768d8d17bb785e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"graft\"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]', 'graft', '[SEP]']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}