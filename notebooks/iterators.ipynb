{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from finetune_vs_scratch import dataset as ds\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%%writefile file1.txt\n",
    "file1 tweet1\n",
    "file1 tweet2\n",
    "file1 tweet3\n",
    "file1 tweet4"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting file1.txt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "%%writefile file2.txt\n",
    "file2 tweet1\n",
    "file2 tweet2\n",
    "file2 tweet3\n",
    "file2 tweet4\n",
    "file2 tweet5"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting file2.txt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../models/twerto-base-uncased\")\n",
    "\n",
    "dataset = ds.BatchProcessedDataset(\n",
    "    [\"file1.txt\", \"file2.txt\"], tokenizer, batch_size=7\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "for _, line in zip(range(20), dataset.lines):\n",
    "    print(line)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "file1 tweet1\n",
      "file1 tweet2\n",
      "file1 tweet3\n",
      "file1 tweet4\n",
      "file2 tweet1\n",
      "file2 tweet2\n",
      "file2 tweet3\n",
      "file2 tweet4\n",
      "file2 tweet5\n",
      "file1 tweet1\n",
      "file1 tweet2\n",
      "file1 tweet3\n",
      "file1 tweet4\n",
      "file2 tweet1\n",
      "file2 tweet2\n",
      "file2 tweet3\n",
      "file2 tweet4\n",
      "file2 tweet5\n",
      "file1 tweet1\n",
      "file1 tweet2\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "batches = dataset.batches\n",
    "\n",
    "list(next(batches))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['file1 tweet1',\n",
       " 'file1 tweet2',\n",
       " 'file1 tweet3',\n",
       " 'file1 tweet4',\n",
       " 'file2 tweet1',\n",
       " 'file2 tweet2',\n",
       " 'file2 tweet3']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "list(next(batches))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['file2 tweet4',\n",
       " 'file2 tweet5',\n",
       " 'file1 tweet1',\n",
       " 'file1 tweet2',\n",
       " 'file1 tweet3',\n",
       " 'file1 tweet4',\n",
       " 'file2 tweet1']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "list(next(batches))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['file2 tweet2',\n",
       " 'file2 tweet3',\n",
       " 'file2 tweet4',\n",
       " 'file2 tweet5',\n",
       " 'file1 tweet1',\n",
       " 'file1 tweet2',\n",
       " 'file1 tweet3']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "list(next(batches))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['file1 tweet4',\n",
       " 'file2 tweet1',\n",
       " 'file2 tweet2',\n",
       " 'file2 tweet3',\n",
       " 'file2 tweet4',\n",
       " 'file2 tweet5',\n",
       " 'file1 tweet1']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "encoded_batches = dataset.encoded_batches"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "encodings = next(encoded_batches)\n",
    "\n",
    "for enc in encodings:\n",
    "    print(tokenizer.decode(enc[\"input_ids\"]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s> file1 tweet1</s>\n",
      "<s> file1 tweet2</s>\n",
      "<s> file1 tweet3</s>\n",
      "<s> file1 tweet4</s>\n",
      "<s> file2 tweet1</s>\n",
      "<s> file2 tweet2</s>\n",
      "<s> file2 tweet3</s>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "encodings = next(encoded_batches)\n",
    "\n",
    "for enc in encodings:\n",
    "    print(tokenizer.decode(enc[\"input_ids\"]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<s> file2 tweet4</s>\n",
      "<s> file2 tweet5</s>\n",
      "<s> file1 tweet1</s>\n",
      "<s> file1 tweet2</s>\n",
      "<s> file1 tweet3</s>\n",
      "<s> file1 tweet4</s>\n",
      "<s> file2 tweet1</s>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baseline speed"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import glob\n",
    "\n",
    "train_files = glob.glob(\"../data/tweets/train/*\")\n",
    "\n",
    "\n",
    "dataset = ds.BatchProcessedDataset(\n",
    "    train_files, tokenizer, batch_size=1024, limit=1_000_000\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for x in tqdm(dataset):\n",
    "    pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para emular computacionalmente esto:\n",
    "\n",
    "iteramos por "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('finetune-vs-scratch-gHiQbun3-py3.7': poetry)"
  },
  "interpreter": {
   "hash": "caf5cc74d2b3d272f965adf95088ced8e2285640448d25ed8d0409fd68343826"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}