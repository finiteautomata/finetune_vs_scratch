{
    "context_hate": [
        {
            "eval_loss": 0.8265394568443298,
            "eval_calls_f1": 0.0,
            "eval_women_f1": 0.0,
            "eval_lgbti_f1": 0.0,
            "eval_racism_f1": 0.0,
            "eval_class_f1": 0.0,
            "eval_politics_f1": 0.0,
            "eval_disabled_f1": 0.0,
            "eval_appearance_f1": 0.0,
            "eval_criminal_f1": 0.3,
            "eval_mean_f1": 0.03333333507180214,
            "eval_mean_precision": 0.0555555559694767,
            "eval_mean_recall": 0.02380952425301075,
            "eval_hate_precision": 0.25,
            "eval_hate_recall": 0.023255813953488372,
            "eval_hate_f1": 0.0425531914893617,
            "eval_runtime": 3.9251,
            "eval_samples_per_second": 127.387,
            "eval_steps_per_second": 8.153,
            "epoch": 3.0
        },
        {
            "eval_loss": 0.8348337411880493,
            "eval_calls_f1": 0.0,
            "eval_women_f1": 0.0,
            "eval_lgbti_f1": 0.0,
            "eval_racism_f1": 0.0,
            "eval_class_f1": 0.0,
            "eval_politics_f1": 0.14285714285714288,
            "eval_disabled_f1": 0.0,
            "eval_appearance_f1": 0.0,
            "eval_criminal_f1": 0.13333333333333333,
            "eval_mean_f1": 0.030687831342220306,
            "eval_mean_precision": 0.14814814925193787,
            "eval_mean_recall": 0.018037518486380577,
            "eval_hate_precision": 0.0,
            "eval_hate_recall": 0.0,
            "eval_hate_f1": 0.0,
            "eval_runtime": 3.9404,
            "eval_samples_per_second": 126.89,
            "eval_steps_per_second": 8.121,
            "epoch": 3.0
        }
    ],
    "sentiment": [
        {
            "eval_loss": 0.9572370648384094,
            "eval_neg_f1": 0.6782178217821782,
            "eval_neg_precision": 0.6462264150943396,
            "eval_neg_recall": 0.7135416666666666,
            "eval_neu_f1": 0.2818181818181818,
            "eval_neu_precision": 0.5344827586206896,
            "eval_neu_recall": 0.19135802469135801,
            "eval_pos_f1": 0.6329787234042553,
            "eval_pos_precision": 0.5173913043478261,
            "eval_pos_recall": 0.815068493150685,
            "eval_micro_f1": 0.574,
            "eval_macro_f1": 0.5310049057006836,
            "eval_macro_precision": 0.5660334825515747,
            "eval_macro_recall": 0.5733227133750916,
            "eval_acc": 0.574,
            "eval_runtime": 1.8567,
            "eval_samples_per_second": 269.296,
            "eval_steps_per_second": 8.617,
            "epoch": 3.0
        },
        {
            "eval_loss": 0.9572370648384094,
            "eval_neg_f1": 0.6782178217821782,
            "eval_neg_precision": 0.6462264150943396,
            "eval_neg_recall": 0.7135416666666666,
            "eval_neu_f1": 0.2818181818181818,
            "eval_neu_precision": 0.5344827586206896,
            "eval_neu_recall": 0.19135802469135801,
            "eval_pos_f1": 0.6329787234042553,
            "eval_pos_precision": 0.5173913043478261,
            "eval_pos_recall": 0.815068493150685,
            "eval_micro_f1": 0.574,
            "eval_macro_f1": 0.5310049057006836,
            "eval_macro_precision": 0.5660334825515747,
            "eval_macro_recall": 0.5733227133750916,
            "eval_acc": 0.574,
            "eval_runtime": 1.8872,
            "eval_samples_per_second": 264.941,
            "eval_steps_per_second": 8.478,
            "epoch": 3.0
        }
    ],
    "emotion": [
        {
            "eval_loss": 1.7709599733352661,
            "eval_others_f1": 0.44444444444444453,
            "eval_others_precision": 0.7,
            "eval_others_recall": 0.32558139534883723,
            "eval_joy_f1": 0.47302904564315357,
            "eval_joy_precision": 0.4014084507042254,
            "eval_joy_recall": 0.5757575757575758,
            "eval_sadness_f1": 0.4318181818181818,
            "eval_sadness_precision": 0.3131868131868132,
            "eval_sadness_recall": 0.6951219512195121,
            "eval_anger_f1": 0.40384615384615385,
            "eval_anger_precision": 0.42,
            "eval_anger_recall": 0.3888888888888889,
            "eval_surprise_f1": 0.0,
            "eval_surprise_precision": 0.0,
            "eval_surprise_recall": 0.0,
            "eval_disgust_f1": 0.0,
            "eval_disgust_precision": 0.0,
            "eval_disgust_recall": 0.0,
            "eval_fear_f1": 0.0,
            "eval_fear_precision": 0.0,
            "eval_fear_recall": 0.0,
            "eval_micro_f1": 0.41,
            "eval_macro_f1": 0.2504482567310333,
            "eval_macro_precision": 0.26208505034446716,
            "eval_macro_recall": 0.2836213707923889,
            "eval_acc": 0.41,
            "eval_runtime": 1.9263,
            "eval_samples_per_second": 259.564,
            "eval_steps_per_second": 16.612,
            "epoch": 3.0
        },
        {
            "eval_loss": 1.7709599733352661,
            "eval_others_f1": 0.44444444444444453,
            "eval_others_precision": 0.7,
            "eval_others_recall": 0.32558139534883723,
            "eval_joy_f1": 0.47302904564315357,
            "eval_joy_precision": 0.4014084507042254,
            "eval_joy_recall": 0.5757575757575758,
            "eval_sadness_f1": 0.4318181818181818,
            "eval_sadness_precision": 0.3131868131868132,
            "eval_sadness_recall": 0.6951219512195121,
            "eval_anger_f1": 0.40384615384615385,
            "eval_anger_precision": 0.42,
            "eval_anger_recall": 0.3888888888888889,
            "eval_surprise_f1": 0.0,
            "eval_surprise_precision": 0.0,
            "eval_surprise_recall": 0.0,
            "eval_disgust_f1": 0.0,
            "eval_disgust_precision": 0.0,
            "eval_disgust_recall": 0.0,
            "eval_fear_f1": 0.0,
            "eval_fear_precision": 0.0,
            "eval_fear_recall": 0.0,
            "eval_micro_f1": 0.41,
            "eval_macro_f1": 0.2504482567310333,
            "eval_macro_precision": 0.26208505034446716,
            "eval_macro_recall": 0.2836213707923889,
            "eval_acc": 0.41,
            "eval_runtime": 1.9508,
            "eval_samples_per_second": 256.299,
            "eval_steps_per_second": 16.403,
            "epoch": 3.0
        }
    ],
    "model_name": "dccuchile/bert-base-spanish-wwm-uncased"
}