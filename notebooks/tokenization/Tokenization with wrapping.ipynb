{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(tweet_files[0])])[:1_00_000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from tokenizers import SentencePieceBPETokenizer, BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers import normalizers, Regex\n",
    "from tokenizers.processors import RobertaProcessing\n",
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()\n",
    "#replacement=\"_\")\n",
    "\n",
    "tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "\n",
    "strip_accents = True\n",
    "lowercase = True\n",
    "\n",
    "tokenizer_normalizers = [\n",
    "    normalizers.NFKC(),\n",
    "    normalizers.BertNormalizer(\n",
    "        clean_text=True,\n",
    "        handle_chinese_chars=True,\n",
    "        strip_accents=strip_accents,\n",
    "        lowercase=lowercase,\n",
    "    ),\n",
    "    normalizers.Replace(Regex(\"(\\W)?@usuario(\\W)\"), \" @usuario \"),\n",
    "    normalizers.Replace(\"hashtag\", \" hashtag \"),\n",
    "    # Error de preprocesamiento\n",
    "    normalizers.Replace(Regex(\"(\\W)url(\\W)\"), \" url \"),\n",
    "    normalizers.Replace(\"http://url\", \" url \"),\n",
    "]\n",
    "\n",
    "tokenizer.normalizer = normalizers.Sequence(tokenizer_normalizers)\n",
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "tokenizer.post_processor = RobertaProcessing(\n",
    "    cls=(\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "    sep=(\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "#tokenizer.add_tokens(special_tokens)\n",
    "\n",
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    limit_alphabet=300,\n",
    "    special_tokens=tokenizer_special_tokens+special_tokens,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "tokenizer.normalizer.normalize_str(\"@usuariotugo\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'@usuariotugo'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "tokenizer.normalizer.normalize_str(\"..url..\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'. url .'"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\n",
    "for tok in tokenizer.get_vocab():\n",
    "    if any(t in tok for t in special_tokens):\n",
    "        print(tok)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "‚ñÅ@usuario\n",
      "@usuario\n",
      "url\n",
      "‚ñÅburlarse\n",
      "emoji\n",
      "‚ñÅurl\n",
      "‚ñÅburlar\n",
      "‚ñÅburlandose\n",
      "@url\n",
      "‚ñÅburla\n",
      "‚ñÅurl.\n",
      "‚ñÅemoji\n",
      "‚ñÅburlan\n",
      "‚ñÅhashtag\n",
      "hashtag\n",
      "‚ñÅemojis\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Alphabet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "inv_vocab = {v:k for k, v in vocab.items()}\n",
    "inv_vocab = [inv_vocab[i] for i in range(len(inv_vocab)) if i not in {335, 2388, 3075}]\n",
    "\n",
    "print(f\"First tokens: {inv_vocab[:50]}\")\n",
    "\n",
    "alphabet = sorted(list({a for x in tokenizer.get_vocab() for a in x}))\n",
    "print(\"Alphabet = \", \" \".join(alphabet))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "First tokens: ['<s>', '<pad>', '</s>', '<unk>', '<mask>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i']\n",
      "Alphabet =  ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ [ \\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z | ~ ¬° ¬¨ ¬Æ ¬ø …™ …¥  Ä  ü Œ± Œµ Œ∑ Œπ ŒΩ Œø œÄ œÅ œÉ œÖ –≤ –¥ –∫ –Ω –æ —Ç ◊ê ◊î ◊ï ◊ô ◊ú ◊® ◊© ◊™ ÿß ÿ® ÿ™ ÿ≠ ÿØ ÿ± ÿ≥ ÿ¥ ÿπ ŸÅ ŸÇ ŸÉ ŸÑ ŸÖ ŸÜ Ÿá Ÿà Ÿä ·ÑÄ ·ÑÅ ·ÑÇ ·ÑÉ ·ÑÑ ·ÑÖ ·ÑÜ ·Ñá ·Ñâ ·Ñä ·Ñã ·Ñå ·Ñé ·Ñè ·Ñê ·Ñë ·Ñí ·Ö° ·Ö¢ ·Ö£ ·Ö• ·Ö¶ ·Öß ·Ö© ·Ö™ ·Ö¨ ·Ö≠ ·ÖÆ ·ÖØ ·Ö± ·Ö≤ ·Ö≥ ·Ö¥ ·Öµ ·Ü® ·Ü´ ·Ü≠ ·ÜØ ·Ü∑ ·Ü∏ ·Üπ ·Ü∫ ·Üª ·Üº ·áÇ ·•± ·¥Ä ·¥á ·¥ç ·¥è ·¥õ ·¥ú ·µé ‚É£ ‚Üí ‚è± ‚ñÅ ‚ñ∫ ‚òÖ ‚òÜ ‚ô™ ‚ôª ‚úî ‚û° ‚ûΩ ‚†Ä ‚¨á „ÄÅ „ÄÇ „Äå „Äç „Äé „Äè „ÅÇ „ÅÑ „ÅÜ „Åà „Åä „Åã „Åç „Åè „Åë „Åì „Åï „Åó „Åô „Åõ „Åù „Åü „Å° „Å£ „Å§ „Å¶ „Å® „Å™ „Å´ „Å≠ „ÅÆ „ÅØ „Å≤ „Åµ „Åª „Åæ „Åø „ÇÇ „ÇÉ „ÇÑ „Çà „Çâ „Çä „Çã „Çå „Çç „Çè „Çí „Çì „Ç¢ „Ç£ „Ç§ „Ç¶ „Ç® „Ç´ „Ç≠ „ÇØ „Ç± „Ç≥ „Çµ „Ç∑ „Çπ „Çø „ÉÅ „ÉÉ „ÉÑ „ÉÜ „Éà „Éè „Éí „Éï „Éò „Éû „Éü „É° „É£ „É© „É™ „É´ „É¨ „É≠ „É≥ „Éª ‰∏Ä ‰∏≠ ‰∫∫ ‰ªä ‰Ωï ÂÖà ÂàÜ Ââ£ Âãù Âíå Â§ú Â§ß Â∫¶ Êèè Êèê Êó• Êúà Êú¨ Ê∞ó Ê∞¥ Ê∑± Áâà Áîü Áîª Áúü ÁùÄ Áù£ ÁßÅ Áµµ Ëâ¶ Ë¶ã Ë≤† Ë≤º üéô üëÅ üó£\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "tokenizer.encode(\"@usuario son UNA MIERDA\", \"Viva Per√≥n\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Encoding(num_tokens=10, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "tokenizer.decode(tokenizer.encode(\"@usuario son UNA MIERDA\", \"Viva Per√≥n\").ids)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'@usuario son una mierda viva peron'"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "transformer_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    sep_token=\"</s>\",\n",
    "    cls_token=\"<s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    mask_token=\"<mask>\",\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "\n",
    "transformer_tokenizer.save_pretrained(\"small\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('small/tokenizer_config.json',\n",
       " 'small/special_tokens_map.json',\n",
       " 'small/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "from transformers import AutoTokenizer\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(\"small\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'bos_token': '<s>', 'eos_token': '</s>', 'sep_token': '</s>', 'cls_token': '<s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'special_tokens_map_file': 'small/special_tokens_map.json', 'tokenizer_file': 'small/tokenizer.json', 'name_or_path': 'small'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "transformer_tokenizer._tokenizer.encode(\"Este es un forro @usuario impresion√°nte\", \"Corte gil corte basura\").tokens\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '‚ñÅeste',\n",
       " '‚ñÅes',\n",
       " '‚ñÅun',\n",
       " '‚ñÅforro',\n",
       " '‚ñÅ',\n",
       " '@usuario',\n",
       " '‚ñÅimpresionante',\n",
       " '</s>',\n",
       " '</s>',\n",
       " '‚ñÅcorte',\n",
       " '‚ñÅgil',\n",
       " '‚ñÅcorte',\n",
       " '‚ñÅbasura',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'model_max_length': 512, 'vocab_file': '/home/jmperez/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab', 'merges_file': '/home/jmperez/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b', 'tokenizer_file': '/home/jmperez/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730', 'special_tokens_map_file': None, 'name_or_path': 'roberta-base'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "\n",
    "tokenizer._tokenizer.encode(\"Oh man this is terrible\", \"Bullshit\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Oh',\n",
       " 'ƒ†man',\n",
       " 'ƒ†this',\n",
       " 'ƒ†is',\n",
       " 'ƒ†terrible',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'Bull',\n",
       " 'shit',\n",
       " '</s>']"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test pretrained\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizers = {\n",
    "    \"deacc\": \"../../models/twerto-base-deacc-uncased\",\n",
    "    \"uncased\": \"../../models/twerto-base-uncased\",\n",
    "    \"cased\": \"../../models/twerto-base-cased\",\n",
    "}\n",
    "\n",
    "tokenizers = {k: AutoTokenizer.from_pretrained(v) for k, v in tokenizers.items()}\n",
    "\n",
    "for model_name, tokenizer in tokenizers.items():\n",
    "    print(\"=\"*80)\n",
    "    print(model_name, \"\\n\"*3)\n",
    "    print(\"Sanity check\")\n",
    "    print(f\"@usuario => {tokenizer.encode('@usuario')}\")\n",
    "    text = [\"esta es una PRUEBA EN MAY√öSCULAS Y CON TILDES @usuario @usuario\", \"ATR cumbia gato hashtag\"]\n",
    "    print(f\"{text}\\n{tokenizer.decode(tokenizer.encode(*text))}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "================================================================================\n",
      "deacc \n",
      "\n",
      "\n",
      "\n",
      "Sanity check\n",
      "@usuario => [0, 433, 2]\n",
      "['esta es una PRUEBA EN MAY√öSCULAS Y CON TILDES @usuario @usuario', 'ATR cumbia gato hashtag']\n",
      "<s> esta es una prueba en mayusculas y con tildes @usuario @usuario</s></s> atr cumbia gato  hashtag </s>\n",
      "================================================================================\n",
      "uncased \n",
      "\n",
      "\n",
      "\n",
      "Sanity check\n",
      "@usuario => [0, 431, 2]\n",
      "['esta es una PRUEBA EN MAY√öSCULAS Y CON TILDES @usuario @usuario', 'ATR cumbia gato hashtag']\n",
      "<s> esta es una prueba en may√∫sculas y con tildes @usuario @usuario</s></s> atr cumbia gato  hashtag </s>\n",
      "================================================================================\n",
      "cased \n",
      "\n",
      "\n",
      "\n",
      "Sanity check\n",
      "@usuario => [0, 430, 2]\n",
      "['esta es una PRUEBA EN MAY√öSCULAS Y CON TILDES @usuario @usuario', 'ATR cumbia gato hashtag']\n",
      "<s> esta es una PRUEBA EN MAY√öSCULAS Y CON TILDES @usuario @usuario</s></s> ATR cumbia gato  hashtag </s>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenizer = tokenizers[\"deacc\"]\n",
    "\n",
    "inv_vocab = {v:k for k, v in tokenizer.vocab.items()}\n",
    "\n",
    "with open(\"deacc_vocab.txt\", \"w+\") as f:\n",
    "    for i in range(len(inv_vocab)):\n",
    "        f.write(f\"{i:<6} --- {inv_vocab[i]}\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}