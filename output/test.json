{
    "context_hate": [
        {
            "eval_loss": 0.3497675061225891,
            "eval_calls_f1": 0.18181818181818182,
            "eval_women_f1": 0.0,
            "eval_lgbti_f1": 0.0,
            "eval_racism_f1": 0.0,
            "eval_class_f1": 0.0,
            "eval_politics_f1": 0.0,
            "eval_disabled_f1": 0.0,
            "eval_appearance_f1": 0.0,
            "eval_criminal_f1": 0.41666666666666663,
            "eval_mean_f1": 0.06649831682443619,
            "eval_mean_precision": 0.07575757801532745,
            "eval_mean_recall": 0.05988456308841705,
            "eval_hate_precision": 0.17647058823529413,
            "eval_hate_recall": 0.11538461538461539,
            "eval_hate_f1": 0.13953488372093026,
            "eval_runtime": 1.4315,
            "eval_samples_per_second": 349.295,
            "eval_steps_per_second": 22.355
        },
        {
            "eval_loss": 0.34915220737457275,
            "eval_calls_f1": 0.0,
            "eval_women_f1": 0.0,
            "eval_lgbti_f1": 0.0,
            "eval_racism_f1": 0.0,
            "eval_class_f1": 0.0,
            "eval_politics_f1": 0.0,
            "eval_disabled_f1": 0.0,
            "eval_appearance_f1": 0.0,
            "eval_criminal_f1": 0.5106382978723404,
            "eval_mean_f1": 0.05673758685588837,
            "eval_mean_precision": 0.04040404036641121,
            "eval_mean_recall": 0.095238097012043,
            "eval_hate_precision": 0.2826086956521739,
            "eval_hate_recall": 0.25,
            "eval_hate_f1": 0.2653061224489796,
            "eval_runtime": 1.4585,
            "eval_samples_per_second": 342.827,
            "eval_steps_per_second": 21.941
        }
    ],
    "sentiment": [
        {
            "eval_loss": 0.9173914790153503,
            "eval_neg_f1": 0.6885964912280702,
            "eval_neg_precision": 0.5946969696969697,
            "eval_neg_recall": 0.8177083333333334,
            "eval_neu_f1": 0.2545454545454546,
            "eval_neu_precision": 0.4827586206896552,
            "eval_neu_recall": 0.1728395061728395,
            "eval_pos_f1": 0.6296296296296295,
            "eval_pos_precision": 0.5730337078651685,
            "eval_pos_recall": 0.6986301369863014,
            "eval_micro_f1": 0.574,
            "eval_macro_f1": 0.5242571830749512,
            "eval_macro_precision": 0.5501630902290344,
            "eval_macro_recall": 0.5630593299865723,
            "eval_acc": 0.574,
            "eval_runtime": 0.5406,
            "eval_samples_per_second": 924.937,
            "eval_steps_per_second": 29.598,
            "epoch": 3.0
        },
        {
            "eval_loss": 0.9222723245620728,
            "eval_neg_f1": 0.6819221967963386,
            "eval_neg_precision": 0.6081632653061224,
            "eval_neg_recall": 0.7760416666666666,
            "eval_neu_f1": 0.28947368421052627,
            "eval_neu_precision": 0.5,
            "eval_neu_recall": 0.2037037037037037,
            "eval_pos_f1": 0.6328358208955224,
            "eval_pos_precision": 0.5608465608465608,
            "eval_pos_recall": 0.726027397260274,
            "eval_micro_f1": 0.576,
            "eval_macro_f1": 0.5347438454627991,
            "eval_macro_precision": 0.5563365817070007,
            "eval_macro_recall": 0.5685909390449524,
            "eval_acc": 0.576,
            "eval_runtime": 0.5433,
            "eval_samples_per_second": 920.297,
            "eval_steps_per_second": 29.45,
            "epoch": 3.0
        }
    ],
    "emotion": [
        {
            "eval_loss": 1.5966269969940186,
            "eval_others_f1": 0.3391003460207612,
            "eval_others_precision": 0.6621621621621622,
            "eval_others_recall": 0.22790697674418606,
            "eval_joy_f1": 0.5296442687747035,
            "eval_joy_precision": 0.43506493506493504,
            "eval_joy_recall": 0.6767676767676768,
            "eval_sadness_f1": 0.5700934579439252,
            "eval_sadness_precision": 0.4621212121212121,
            "eval_sadness_recall": 0.7439024390243902,
            "eval_anger_f1": 0.3529411764705882,
            "eval_anger_precision": 0.3230769230769231,
            "eval_anger_recall": 0.3888888888888889,
            "eval_surprise_f1": 0.27272727272727276,
            "eval_surprise_precision": 0.2647058823529412,
            "eval_surprise_recall": 0.28125,
            "eval_disgust_f1": 0.0,
            "eval_disgust_precision": 0.0,
            "eval_disgust_recall": 0.0,
            "eval_fear_f1": 0.0,
            "eval_fear_precision": 0.0,
            "eval_fear_recall": 0.0,
            "eval_micro_f1": 0.414,
            "eval_macro_f1": 0.29492950439453125,
            "eval_macro_precision": 0.30673304200172424,
            "eval_macro_recall": 0.3312451243400574,
            "eval_acc": 0.414,
            "eval_runtime": 1.0778,
            "eval_samples_per_second": 463.907,
            "eval_steps_per_second": 29.69,
            "epoch": 3.0
        },
        {
            "eval_loss": 1.6937607526779175,
            "eval_others_f1": 0.492401215805471,
            "eval_others_precision": 0.7105263157894737,
            "eval_others_recall": 0.3767441860465116,
            "eval_joy_f1": 0.5611510791366906,
            "eval_joy_precision": 0.43575418994413406,
            "eval_joy_recall": 0.7878787878787878,
            "eval_sadness_f1": 0.5388127853881278,
            "eval_sadness_precision": 0.4306569343065693,
            "eval_sadness_recall": 0.7195121951219512,
            "eval_anger_f1": 0.35714285714285715,
            "eval_anger_precision": 0.3448275862068966,
            "eval_anger_recall": 0.37037037037037035,
            "eval_surprise_f1": 0.15789473684210525,
            "eval_surprise_precision": 0.5,
            "eval_surprise_recall": 0.09375,
            "eval_disgust_f1": 0.0,
            "eval_disgust_precision": 0.0,
            "eval_disgust_recall": 0.0,
            "eval_fear_f1": 0.0,
            "eval_fear_precision": 0.0,
            "eval_fear_recall": 0.0,
            "eval_micro_f1": 0.482,
            "eval_macro_f1": 0.30105751752853394,
            "eval_macro_precision": 0.3459664285182953,
            "eval_macro_recall": 0.33546510338783264,
            "eval_acc": 0.482,
            "eval_runtime": 1.0786,
            "eval_samples_per_second": 463.562,
            "eval_steps_per_second": 29.668,
            "epoch": 3.0
        }
    ],
    "model_name": "dccuchile/bert-base-spanish-wwm-uncased"
}