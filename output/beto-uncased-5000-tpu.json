{
    "context_hate": [
        {
            "eval_loss": 0.07834901660680771,
            "eval_calls_f1": 0.6826758147512864,
            "eval_women_f1": 0.4173913043478261,
            "eval_lgbti_f1": 0.49704142011834324,
            "eval_racism_f1": 0.6692836113837095,
            "eval_class_f1": 0.4899328859060403,
            "eval_politics_f1": 0.61139896373057,
            "eval_disabled_f1": 0.555984555984556,
            "eval_appearance_f1": 0.7451487710219921,
            "eval_criminal_f1": 0.6266094420600858,
            "eval_mean_f1": 0.5883852243423462,
            "eval_mean_precision": 0.6048446893692017,
            "eval_mean_recall": 0.5785748958587646,
            "eval_hate_precision": 0.6966033390903857,
            "eval_hate_recall": 0.6733444629938787,
            "eval_hate_f1": 0.6847764572722128,
            "eval_runtime": 33.512,
            "eval_samples_per_second": 338.476,
            "eval_steps_per_second": 21.157
        },
        {
            "eval_loss": 0.07917915284633636,
            "eval_calls_f1": 0.6362038664323374,
            "eval_women_f1": 0.4101479915433404,
            "eval_lgbti_f1": 0.4793388429752066,
            "eval_racism_f1": 0.6639676113360323,
            "eval_class_f1": 0.5211267605633804,
            "eval_politics_f1": 0.6161790017211703,
            "eval_disabled_f1": 0.5842696629213482,
            "eval_appearance_f1": 0.7347994825355757,
            "eval_criminal_f1": 0.6246418338108882,
            "eval_mean_f1": 0.5856305360794067,
            "eval_mean_precision": 0.5990739464759827,
            "eval_mean_recall": 0.5758101940155029,
            "eval_hate_precision": 0.7023391812865497,
            "eval_hate_recall": 0.6683361157484696,
            "eval_hate_f1": 0.6849158825206729,
            "eval_runtime": 33.5239,
            "eval_samples_per_second": 338.356,
            "eval_steps_per_second": 21.149
        },
        {
            "eval_loss": 0.08049201965332031,
            "eval_calls_f1": 0.6829268292682927,
            "eval_women_f1": 0.427061310782241,
            "eval_lgbti_f1": 0.47507331378299117,
            "eval_racism_f1": 0.673040152963671,
            "eval_class_f1": 0.4893617021276596,
            "eval_politics_f1": 0.6059602649006622,
            "eval_disabled_f1": 0.5229681978798587,
            "eval_appearance_f1": 0.7254658385093169,
            "eval_criminal_f1": 0.6362339514978602,
            "eval_mean_f1": 0.5820101499557495,
            "eval_mean_precision": 0.5879033207893372,
            "eval_mean_recall": 0.5837166905403137,
            "eval_hate_precision": 0.6778819635962493,
            "eval_hate_recall": 0.6839176405119644,
            "eval_hate_f1": 0.6808864265927977,
            "eval_runtime": 33.4781,
            "eval_samples_per_second": 338.818,
            "eval_steps_per_second": 21.178
        }
    ],
    "hate": [
        {
            "eval_loss": 0.5549820065498352,
            "eval_ok_f1": 0.7562130177514792,
            "eval_ok_precision": 0.852,
            "eval_ok_recall": 0.6797872340425531,
            "eval_hateful_f1": 0.7271523178807948,
            "eval_hateful_precision": 0.6458823529411765,
            "eval_hateful_recall": 0.8318181818181818,
            "eval_micro_f1": 0.7425,
            "eval_macro_f1": 0.7416826486587524,
            "eval_macro_precision": 0.74894118309021,
            "eval_macro_recall": 0.7558026909828186,
            "eval_acc": 0.7425,
            "eval_runtime": 3.0484,
            "eval_samples_per_second": 524.858,
            "eval_steps_per_second": 32.804,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.325300931930542,
            "eval_ok_f1": 0.7895953757225435,
            "eval_ok_precision": 0.8645569620253165,
            "eval_ok_recall": 0.7265957446808511,
            "eval_hateful_f1": 0.7523809523809524,
            "eval_hateful_precision": 0.682716049382716,
            "eval_hateful_recall": 0.8378787878787879,
            "eval_micro_f1": 0.7725000000000001,
            "eval_macro_f1": 0.7709881663322449,
            "eval_macro_precision": 0.773636519908905,
            "eval_macro_recall": 0.7822372913360596,
            "eval_acc": 0.7725,
            "eval_runtime": 3.0596,
            "eval_samples_per_second": 522.951,
            "eval_steps_per_second": 32.684,
            "epoch": 5.0
        },
        {
            "eval_loss": 0.5603035688400269,
            "eval_ok_f1": 0.7852906287069988,
            "eval_ok_precision": 0.8873994638069705,
            "eval_ok_recall": 0.7042553191489361,
            "eval_hateful_f1": 0.7608982826948482,
            "eval_hateful_precision": 0.6744730679156908,
            "eval_hateful_recall": 0.8727272727272727,
            "eval_micro_f1": 0.77375,
            "eval_macro_f1": 0.7730944752693176,
            "eval_macro_precision": 0.7809362411499023,
            "eval_macro_recall": 0.7884913086891174,
            "eval_acc": 0.77375,
            "eval_runtime": 3.045,
            "eval_samples_per_second": 525.458,
            "eval_steps_per_second": 32.841,
            "epoch": 5.0
        }
    ],
    "sentiment": [
        {
            "eval_loss": 1.387026309967041,
            "eval_neg_f1": 0.7329376854599406,
            "eval_neg_precision": 0.8055442315532002,
            "eval_neg_recall": 0.6723375297720313,
            "eval_neu_f1": 0.5446735395189003,
            "eval_neu_precision": 0.47137546468401487,
            "eval_neu_recall": 0.6449643947100712,
            "eval_pos_f1": 0.7339285714285714,
            "eval_pos_precision": 0.7751060820367751,
            "eval_pos_recall": 0.6969054684188215,
            "eval_micro_f1": 0.6729074889867841,
            "eval_macro_f1": 0.6705133318901062,
            "eval_macro_precision": 0.6840085983276367,
            "eval_macro_recall": 0.6714024543762207,
            "eval_acc": 0.6729074889867841,
            "eval_runtime": 9.3357,
            "eval_samples_per_second": 778.092,
            "eval_steps_per_second": 48.631,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.0981433391571045,
            "eval_neg_f1": 0.7329636916472902,
            "eval_neg_precision": 0.7726244343891403,
            "eval_neg_recall": 0.6971759101735284,
            "eval_neu_f1": 0.5310119695321001,
            "eval_neu_precision": 0.46405477367820464,
            "eval_neu_recall": 0.6205493387589013,
            "eval_pos_f1": 0.7240902809765085,
            "eval_pos_precision": 0.7927382753403933,
            "eval_pos_recall": 0.6663840610428148,
            "eval_micro_f1": 0.6664372246696035,
            "eval_macro_f1": 0.662688672542572,
            "eval_macro_precision": 0.6764724850654602,
            "eval_macro_recall": 0.6613697409629822,
            "eval_acc": 0.6664372246696035,
            "eval_runtime": 9.3114,
            "eval_samples_per_second": 780.123,
            "eval_steps_per_second": 48.758,
            "epoch": 5.0
        },
        {
            "eval_loss": 0.7945075631141663,
            "eval_neg_f1": 0.7654278895329014,
            "eval_neg_precision": 0.7669969251793646,
            "eval_neg_recall": 0.7638652602926166,
            "eval_neu_f1": 0.5398187887458274,
            "eval_neu_precision": 0.5080789946140036,
            "eval_neu_recall": 0.5757884028484231,
            "eval_pos_f1": 0.7488809310653536,
            "eval_pos_precision": 0.793266951161688,
            "eval_pos_recall": 0.7091988130563798,
            "eval_micro_f1": 0.6952092511013216,
            "eval_macro_f1": 0.6847091317176819,
            "eval_macro_precision": 0.6894476413726807,
            "eval_macro_recall": 0.6829507946968079,
            "eval_acc": 0.6952092511013216,
            "eval_runtime": 9.3215,
            "eval_samples_per_second": 779.275,
            "eval_steps_per_second": 48.705,
            "epoch": 5.0
        }
    ],
    "emotion": [
        {
            "eval_loss": 1.6202383041381836,
            "eval_others_f1": 0.7737135771853688,
            "eval_others_precision": 0.7938931297709924,
            "eval_others_recall": 0.75453446191052,
            "eval_joy_f1": 0.6905710491367862,
            "eval_joy_precision": 0.6649616368286445,
            "eval_joy_recall": 0.7182320441988951,
            "eval_sadness_f1": 0.7596153846153847,
            "eval_sadness_precision": 0.728110599078341,
            "eval_sadness_recall": 0.7939698492462312,
            "eval_anger_f1": 0.5795454545454546,
            "eval_anger_precision": 0.5604395604395604,
            "eval_anger_recall": 0.6,
            "eval_surprise_f1": 0.4159999999999999,
            "eval_surprise_precision": 0.45614035087719296,
            "eval_surprise_recall": 0.38235294117647056,
            "eval_disgust_f1": 0.0425531914893617,
            "eval_disgust_precision": 0.07142857142857142,
            "eval_disgust_recall": 0.030303030303030304,
            "eval_fear_f1": 0.5416666666666666,
            "eval_fear_precision": 0.43333333333333335,
            "eval_fear_recall": 0.7222222222222222,
            "eval_micro_f1": 0.7060226595110316,
            "eval_macro_f1": 0.5433807969093323,
            "eval_macro_precision": 0.5297581553459167,
            "eval_macro_recall": 0.5716592073440552,
            "eval_acc": 0.7060226595110316,
            "eval_runtime": 3.5203,
            "eval_samples_per_second": 476.38,
            "eval_steps_per_second": 29.827,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.6137347221374512,
            "eval_others_f1": 0.7654320987654321,
            "eval_others_precision": 0.7818411097099621,
            "eval_others_recall": 0.7496977025392987,
            "eval_joy_f1": 0.6911957950065702,
            "eval_joy_precision": 0.6591478696741855,
            "eval_joy_recall": 0.7265193370165746,
            "eval_sadness_f1": 0.7769423558897244,
            "eval_sadness_precision": 0.775,
            "eval_sadness_recall": 0.7788944723618091,
            "eval_anger_f1": 0.5888888888888889,
            "eval_anger_precision": 0.5578947368421052,
            "eval_anger_recall": 0.6235294117647059,
            "eval_surprise_f1": 0.4,
            "eval_surprise_precision": 0.48936170212765956,
            "eval_surprise_recall": 0.3382352941176471,
            "eval_disgust_f1": 0.23529411764705885,
            "eval_disgust_precision": 0.3333333333333333,
            "eval_disgust_recall": 0.18181818181818182,
            "eval_fear_f1": 0.5,
            "eval_fear_precision": 0.4,
            "eval_fear_recall": 0.6666666666666666,
            "eval_micro_f1": 0.7066189624329159,
            "eval_macro_f1": 0.565393328666687,
            "eval_macro_precision": 0.570939838886261,
            "eval_macro_recall": 0.5807658433914185,
            "eval_acc": 0.7066189624329159,
            "eval_runtime": 3.5271,
            "eval_samples_per_second": 475.467,
            "eval_steps_per_second": 29.77,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.4164267778396606,
            "eval_others_f1": 0.7621451104100947,
            "eval_others_precision": 0.7968337730870713,
            "eval_others_recall": 0.7303506650544136,
            "eval_joy_f1": 0.6826666666666666,
            "eval_joy_precision": 0.6597938144329897,
            "eval_joy_recall": 0.7071823204419889,
            "eval_sadness_f1": 0.742014742014742,
            "eval_sadness_precision": 0.7259615384615384,
            "eval_sadness_recall": 0.7587939698492462,
            "eval_anger_f1": 0.5888594164456235,
            "eval_anger_precision": 0.5362318840579711,
            "eval_anger_recall": 0.6529411764705882,
            "eval_surprise_f1": 0.4397163120567376,
            "eval_surprise_precision": 0.4246575342465753,
            "eval_surprise_recall": 0.45588235294117646,
            "eval_disgust_f1": 0.13333333333333333,
            "eval_disgust_precision": 0.25,
            "eval_disgust_recall": 0.09090909090909091,
            "eval_fear_f1": 0.5714285714285714,
            "eval_fear_precision": 0.45161290322580644,
            "eval_fear_recall": 0.7777777777777778,
            "eval_micro_f1": 0.6976744186046512,
            "eval_macro_f1": 0.5600234866142273,
            "eval_macro_precision": 0.5492988228797913,
            "eval_macro_recall": 0.596262514591217,
            "eval_acc": 0.6976744186046512,
            "eval_runtime": 3.5389,
            "eval_samples_per_second": 473.874,
            "eval_steps_per_second": 29.67,
            "epoch": 5.0
        }
    ],
    "model_name": "models/beto-uncased-5000-tpu/"
}