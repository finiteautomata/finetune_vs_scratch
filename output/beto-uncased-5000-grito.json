{
    "context_hate": [
        {
            "eval_loss": 0.08149804174900055,
            "eval_calls_f1": 0.6469565217391303,
            "eval_women_f1": 0.4250474383301708,
            "eval_lgbti_f1": 0.4406779661016949,
            "eval_racism_f1": 0.6793206793206792,
            "eval_class_f1": 0.46621621621621623,
            "eval_politics_f1": 0.6185567010309279,
            "eval_disabled_f1": 0.5441696113074205,
            "eval_appearance_f1": 0.7146282973621103,
            "eval_criminal_f1": 0.6306306306306306,
            "eval_mean_f1": 0.574022650718689,
            "eval_mean_precision": 0.5737402439117432,
            "eval_mean_recall": 0.5790188908576965,
            "eval_hate_precision": 0.6683087027914614,
            "eval_hate_recall": 0.679465776293823,
            "eval_hate_f1": 0.673841059602649,
            "eval_runtime": 33.4428,
            "eval_samples_per_second": 339.176,
            "eval_steps_per_second": 21.2
        },
        {
            "eval_loss": 0.07911061495542526,
            "eval_calls_f1": 0.6666666666666666,
            "eval_women_f1": 0.4096385542168674,
            "eval_lgbti_f1": 0.4598930481283422,
            "eval_racism_f1": 0.7098703888334994,
            "eval_class_f1": 0.48135593220338985,
            "eval_politics_f1": 0.6380165289256199,
            "eval_disabled_f1": 0.55,
            "eval_appearance_f1": 0.7367119901112484,
            "eval_criminal_f1": 0.6432748538011696,
            "eval_mean_f1": 0.5883809328079224,
            "eval_mean_precision": 0.5878422856330872,
            "eval_mean_recall": 0.5922685861587524,
            "eval_hate_precision": 0.695,
            "eval_hate_recall": 0.6961602671118531,
            "eval_hate_f1": 0.69557964970809,
            "eval_runtime": 33.2541,
            "eval_samples_per_second": 341.101,
            "eval_steps_per_second": 21.321
        },
        {
            "eval_loss": 0.07827892154455185,
            "eval_calls_f1": 0.6642984014209592,
            "eval_women_f1": 0.42399999999999993,
            "eval_lgbti_f1": 0.4804804804804805,
            "eval_racism_f1": 0.6692607003891051,
            "eval_class_f1": 0.50814332247557,
            "eval_politics_f1": 0.6205733558178752,
            "eval_disabled_f1": 0.5663082437275986,
            "eval_appearance_f1": 0.7439490445859872,
            "eval_criminal_f1": 0.6497890295358649,
            "eval_mean_f1": 0.5918669104576111,
            "eval_mean_precision": 0.5956290364265442,
            "eval_mean_recall": 0.5935770273208618,
            "eval_hate_precision": 0.6910206358059119,
            "eval_hate_recall": 0.6894824707846411,
            "eval_hate_f1": 0.6902506963788302,
            "eval_runtime": 33.2972,
            "eval_samples_per_second": 340.659,
            "eval_steps_per_second": 21.293
        }
    ],
    "hate": [
        {
            "eval_loss": 1.1434534788131714,
            "eval_ok_f1": 0.7936507936507936,
            "eval_ok_precision": 0.8869908015768725,
            "eval_ok_recall": 0.7180851063829787,
            "eval_hateful_f1": 0.7658438959306204,
            "eval_hateful_precision": 0.6841477949940405,
            "eval_hateful_recall": 0.8696969696969697,
            "eval_micro_f1": 0.780625,
            "eval_macro_f1": 0.7797473669052124,
            "eval_macro_precision": 0.7855693101882935,
            "eval_macro_recall": 0.7938910722732544,
            "eval_acc": 0.780625,
            "eval_runtime": 3.0155,
            "eval_samples_per_second": 530.594,
            "eval_steps_per_second": 33.162,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.210961937904358,
            "eval_ok_f1": 0.7651006711409396,
            "eval_ok_precision": 0.8969957081545065,
            "eval_ok_recall": 0.6670212765957447,
            "eval_hateful_f1": 0.7533632286995516,
            "eval_hateful_precision": 0.6526082130965594,
            "eval_hateful_recall": 0.8909090909090909,
            "eval_micro_f1": 0.759375,
            "eval_macro_f1": 0.7592319250106812,
            "eval_macro_precision": 0.7748019695281982,
            "eval_macro_recall": 0.778965175151825,
            "eval_acc": 0.759375,
            "eval_runtime": 3.0411,
            "eval_samples_per_second": 526.117,
            "eval_steps_per_second": 32.882,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.1718693971633911,
            "eval_ok_f1": 0.7995365005793743,
            "eval_ok_precision": 0.8778625954198473,
            "eval_ok_recall": 0.7340425531914894,
            "eval_hateful_f1": 0.7652645861601084,
            "eval_hateful_precision": 0.6928746928746928,
            "eval_hateful_recall": 0.8545454545454545,
            "eval_micro_f1": 0.78375,
            "eval_macro_f1": 0.7824005484580994,
            "eval_macro_precision": 0.7853686213493347,
            "eval_macro_recall": 0.794293999671936,
            "eval_acc": 0.78375,
            "eval_runtime": 3.0212,
            "eval_samples_per_second": 529.589,
            "eval_steps_per_second": 33.099,
            "epoch": 5.0
        }
    ],
    "sentiment": [
        {
            "eval_loss": 1.559019684791565,
            "eval_neg_f1": 0.7493761140819964,
            "eval_neg_precision": 0.7869711718457506,
            "eval_neg_recall": 0.715209254848588,
            "eval_neu_f1": 0.5440438156093109,
            "eval_neu_precision": 0.49337748344370863,
            "eval_neu_recall": 0.6063072227873856,
            "eval_pos_f1": 0.7455908289241622,
            "eval_pos_precision": 0.776757005052825,
            "eval_pos_recall": 0.7168291649003815,
            "eval_micro_f1": 0.6862610132158591,
            "eval_macro_f1": 0.6796702742576599,
            "eval_macro_precision": 0.685701847076416,
            "eval_macro_recall": 0.6794486045837402,
            "eval_acc": 0.6862610132158591,
            "eval_runtime": 9.2799,
            "eval_samples_per_second": 782.769,
            "eval_steps_per_second": 48.923,
            "epoch": 5.0
        },
        {
            "eval_loss": 0.7312659621238708,
            "eval_neg_f1": 0.7677353342428378,
            "eval_neg_precision": 0.7695726495726496,
            "eval_neg_recall": 0.7659067710105478,
            "eval_neu_f1": 0.5462721893491124,
            "eval_neu_precision": 0.5108455068614431,
            "eval_neu_recall": 0.5869786368260427,
            "eval_pos_f1": 0.7506195088984006,
            "eval_pos_precision": 0.8009615384615385,
            "eval_pos_recall": 0.7062314540059347,
            "eval_micro_f1": 0.6981002202643172,
            "eval_macro_f1": 0.688209056854248,
            "eval_macro_precision": 0.6937932372093201,
            "eval_macro_recall": 0.6863722801208496,
            "eval_acc": 0.6981002202643172,
            "eval_runtime": 9.2446,
            "eval_samples_per_second": 785.759,
            "eval_steps_per_second": 49.11,
            "epoch": 5.0
        },
        {
            "eval_loss": 0.7796126008033752,
            "eval_neg_f1": 0.756900212314225,
            "eval_neg_precision": 0.7884260965720604,
            "eval_neg_recall": 0.7277985709424974,
            "eval_neu_f1": 0.5562102286118312,
            "eval_neu_precision": 0.51822573561704,
            "eval_neu_recall": 0.6002034587995931,
            "eval_pos_f1": 0.7619253183682279,
            "eval_pos_precision": 0.776165347405453,
            "eval_pos_recall": 0.748198389147944,
            "eval_micro_f1": 0.6998898678414097,
            "eval_macro_f1": 0.691678524017334,
            "eval_macro_precision": 0.6942723393440247,
            "eval_macro_recall": 0.6920668482780457,
            "eval_acc": 0.6998898678414097,
            "eval_runtime": 9.3517,
            "eval_samples_per_second": 776.761,
            "eval_steps_per_second": 48.548,
            "epoch": 5.0
        }
    ],
    "emotion": [
        {
            "eval_loss": 1.678483486175537,
            "eval_others_f1": 0.7676272225628448,
            "eval_others_precision": 0.7786069651741293,
            "eval_others_recall": 0.7569528415961306,
            "eval_joy_f1": 0.6693548387096775,
            "eval_joy_precision": 0.6518324607329843,
            "eval_joy_recall": 0.6878453038674033,
            "eval_sadness_f1": 0.7378640776699029,
            "eval_sadness_precision": 0.7136150234741784,
            "eval_sadness_recall": 0.7638190954773869,
            "eval_anger_f1": 0.6,
            "eval_anger_precision": 0.5684210526315789,
            "eval_anger_recall": 0.6352941176470588,
            "eval_surprise_f1": 0.41071428571428575,
            "eval_surprise_precision": 0.5227272727272727,
            "eval_surprise_recall": 0.3382352941176471,
            "eval_disgust_f1": 0.08,
            "eval_disgust_precision": 0.11764705882352941,
            "eval_disgust_recall": 0.06060606060606061,
            "eval_fear_f1": 0.5777777777777777,
            "eval_fear_precision": 0.48148148148148145,
            "eval_fear_recall": 0.7222222222222222,
            "eval_micro_f1": 0.6994633273703041,
            "eval_macro_f1": 0.5490483641624451,
            "eval_macro_precision": 0.5477616190910339,
            "eval_macro_recall": 0.5664250254631042,
            "eval_acc": 0.6994633273703041,
            "eval_runtime": 3.4672,
            "eval_samples_per_second": 483.673,
            "eval_steps_per_second": 30.284,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.6908990144729614,
            "eval_others_f1": 0.7763558805606338,
            "eval_others_precision": 0.7825552825552825,
            "eval_others_recall": 0.7702539298669892,
            "eval_joy_f1": 0.6804407713498621,
            "eval_joy_precision": 0.6785714285714286,
            "eval_joy_recall": 0.6823204419889503,
            "eval_sadness_f1": 0.7596153846153847,
            "eval_sadness_precision": 0.728110599078341,
            "eval_sadness_recall": 0.7939698492462312,
            "eval_anger_f1": 0.6178861788617886,
            "eval_anger_precision": 0.5728643216080402,
            "eval_anger_recall": 0.6705882352941176,
            "eval_surprise_f1": 0.34234234234234234,
            "eval_surprise_precision": 0.4418604651162791,
            "eval_surprise_recall": 0.27941176470588236,
            "eval_disgust_f1": 0.0425531914893617,
            "eval_disgust_precision": 0.07142857142857142,
            "eval_disgust_recall": 0.030303030303030304,
            "eval_fear_f1": 0.6363636363636364,
            "eval_fear_precision": 0.5384615384615384,
            "eval_fear_recall": 0.7777777777777778,
            "eval_micro_f1": 0.7096004770423375,
            "eval_macro_f1": 0.5507939457893372,
            "eval_macro_precision": 0.5448359847068787,
            "eval_macro_recall": 0.5720893144607544,
            "eval_acc": 0.7096004770423375,
            "eval_runtime": 3.4914,
            "eval_samples_per_second": 480.32,
            "eval_steps_per_second": 30.074,
            "epoch": 5.0
        },
        {
            "eval_loss": 1.5277411937713623,
            "eval_others_f1": 0.7477243172951886,
            "eval_others_precision": 0.8087201125175809,
            "eval_others_recall": 0.6952841596130592,
            "eval_joy_f1": 0.6732429099876696,
            "eval_joy_precision": 0.6080178173719376,
            "eval_joy_recall": 0.7541436464088398,
            "eval_sadness_f1": 0.7684964200477327,
            "eval_sadness_precision": 0.7318181818181818,
            "eval_sadness_recall": 0.8090452261306532,
            "eval_anger_f1": 0.5898876404494383,
            "eval_anger_precision": 0.5645161290322581,
            "eval_anger_recall": 0.6176470588235294,
            "eval_surprise_f1": 0.46280991735537186,
            "eval_surprise_precision": 0.5283018867924528,
            "eval_surprise_recall": 0.4117647058823529,
            "eval_disgust_f1": 0.13559322033898305,
            "eval_disgust_precision": 0.15384615384615385,
            "eval_disgust_recall": 0.12121212121212122,
            "eval_fear_f1": 0.52,
            "eval_fear_precision": 0.40625,
            "eval_fear_recall": 0.7222222222222222,
            "eval_micro_f1": 0.6911150864639237,
            "eval_macro_f1": 0.5568220019340515,
            "eval_macro_precision": 0.5430671572685242,
            "eval_macro_recall": 0.5901884436607361,
            "eval_acc": 0.6911150864639237,
            "eval_runtime": 3.4746,
            "eval_samples_per_second": 482.647,
            "eval_steps_per_second": 30.219,
            "epoch": 5.0
        }
    ],
    "model_name": "models/beto-uncased-5000-grito/"
}