{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/tass.csv\")\n",
    "\n",
    "df"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                       id                                               text  \\\n",
       "0      768512386269638656  @morbosaborealis jajajaja... eso es verdad... ...   \n",
       "1      768529956162924544  @Adriansoler espero y deseo que el interior te...   \n",
       "2      768557093955698688  comprendo que te molen mis tattoos, pero no te...   \n",
       "3      770616744192929792  Mi última partida jugada, con Sona support. La...   \n",
       "4      769959690092642304  Tranquilos que con el.dinero de Camacho seguro...   \n",
       "...                   ...                                                ...   \n",
       "14504  756565904037670912  Un saludo para don Resentidillo muñoz.! Que si...   \n",
       "14505  757602234041524224  Sí, es que se nota que te importa... - Buen in...   \n",
       "14506  786072417307328512  @Isoor_ , @iChibolo , @FloroPeruano , @latiach...   \n",
       "14507  794135682876919808  @bcrpoficial Cash Miguel @canalN_ Manu Ramirez...   \n",
       "14508  785310562729103361  Me encanta tener esa confianza con mi mamá de ...   \n",
       "\n",
       "      lang polarity  split  \n",
       "0       es      NEG  train  \n",
       "1       es      NEU  train  \n",
       "2       es      NEU  train  \n",
       "3       es      POS  train  \n",
       "4       es      POS  train  \n",
       "...    ...      ...    ...  \n",
       "14504   PE      POS   test  \n",
       "14505   PE      NEU   test  \n",
       "14506   PE      NEG   test  \n",
       "14507   PE      NEU   test  \n",
       "14508   PE      POS   test  \n",
       "\n",
       "[14509 rows x 5 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>polarity</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>768512386269638656</td>\n",
       "      <td>@morbosaborealis jajajaja... eso es verdad... ...</td>\n",
       "      <td>es</td>\n",
       "      <td>NEG</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>768529956162924544</td>\n",
       "      <td>@Adriansoler espero y deseo que el interior te...</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>768557093955698688</td>\n",
       "      <td>comprendo que te molen mis tattoos, pero no te...</td>\n",
       "      <td>es</td>\n",
       "      <td>NEU</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>770616744192929792</td>\n",
       "      <td>Mi última partida jugada, con Sona support. La...</td>\n",
       "      <td>es</td>\n",
       "      <td>POS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>769959690092642304</td>\n",
       "      <td>Tranquilos que con el.dinero de Camacho seguro...</td>\n",
       "      <td>es</td>\n",
       "      <td>POS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14504</th>\n",
       "      <td>756565904037670912</td>\n",
       "      <td>Un saludo para don Resentidillo muñoz.! Que si...</td>\n",
       "      <td>PE</td>\n",
       "      <td>POS</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14505</th>\n",
       "      <td>757602234041524224</td>\n",
       "      <td>Sí, es que se nota que te importa... - Buen in...</td>\n",
       "      <td>PE</td>\n",
       "      <td>NEU</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14506</th>\n",
       "      <td>786072417307328512</td>\n",
       "      <td>@Isoor_ , @iChibolo , @FloroPeruano , @latiach...</td>\n",
       "      <td>PE</td>\n",
       "      <td>NEG</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14507</th>\n",
       "      <td>794135682876919808</td>\n",
       "      <td>@bcrpoficial Cash Miguel @canalN_ Manu Ramirez...</td>\n",
       "      <td>PE</td>\n",
       "      <td>NEU</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14508</th>\n",
       "      <td>785310562729103361</td>\n",
       "      <td>Me encanta tener esa confianza con mi mamá de ...</td>\n",
       "      <td>PE</td>\n",
       "      <td>POS</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14509 rows × 5 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from pysentimiento.tass import label2id\n",
    "df[\"label\"] = df[\"polarity\"].apply(lambda x: label2id[x])\n",
    "\n",
    "df[\"split\"].value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "test     7264\n",
       "train    4802\n",
       "dev      2443\n",
       "Name: split, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from transformers import AutoTokenizer\n",
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dccuchile/bert-base-spanish-wwm-uncased\")\n",
    "tokenizer.model_max_length = 128\n",
    "tokenizer.add_tokens(special_tokens)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from glob import glob\n",
    "\n",
    "tweet_files = glob(\"../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:-1]\n",
    "test_files = tweet_files[-1:]\n",
    "len(train_files), len(test_files)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(99, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "\n",
    "features = Features({\n",
    "    'text': Value('string'),\n",
    "})\n",
    "\n",
    "train_dataset, test_dataset = load_dataset(\n",
    "    \"text\", data_files={\"train\": train_files, \"test\": test_files}, split=[\"train\", \"test\"], features=features\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-3b674591494fed6a\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/jmperez/.cache/huggingface/datasets/text/default-3b674591494fed6a/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "46b1c077750f41fa84ff2687f7e3952d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a1a22a995414ba88acd228175202684"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset text downloaded and prepared to /home/jmperez/.cache/huggingface/datasets/text/default-3b674591494fed6a/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "train_dataset.save_to_disk(\"../data/arrow/train\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "test_dataset.save_to_disk(\"../data/arrow/test\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reloading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mirar este issue: https://github.com/huggingface/transformers/issues/10204"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Features, Value, load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk(\"../data/arrow/tokenized_train\")\n",
    "test_dataset = load_from_disk(\"../data/arrow/tokenized_test\")\n",
    "\n",
    "train_dataset, test_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'text', 'token_type_ids'],\n",
       "     num_rows: 25600000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'text', 'token_type_ids'],\n",
       "     num_rows: 5215789\n",
       " }))"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-cased'\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = 128\n",
    "tokenizer.add_tokens(special_tokens)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "example = train_dataset[10900]\n",
    "\n",
    "example[\"text\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Taylor Swift - End Game ft. Ed Sheeran, Future URL via @usuario'"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tokenizer.decode(example[\"input_ids\"])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'[CLS] Taylor Swift - End Game ft. Ed Sheeran, Future URL via @usuario [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tokenizer.save_pretrained(\"../data/beto-tokenizer\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('../data/beto-tokenizer/tokenizer_config.json',\n",
       " '../data/beto-tokenizer/special_tokens_map.json',\n",
       " '../data/beto-tokenizer/vocab.txt',\n",
       " '../data/beto-tokenizer/added_tokens.json',\n",
       " '../data/beto-tokenizer/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tweets = train_dataset[:2048*10][\"text\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": tweets})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, return_special_tokens_mask=True)\n",
    "batch_size = 2048\n",
    "\n",
    "dataset = dataset.map(tokenize, batch_size=batch_size, batched=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d815d0693f564cf4927ea2a38bdcf291"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "dataset.save_to_disk(\"../data/arrow/small_dataset\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "!pwd"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jmperez/projects/finetune_vs_scratch/notebooks\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\n",
    "batch_size = 2048\n",
    "steps = 12_500\n",
    "\n",
    "needed_tweets = batch_size * steps\n",
    "\n",
    "needed_tweets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25600000"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import random\n",
    "\n",
    "random.seed(2021)\n",
    "\n",
    "idx = random.sample(list(range(len(train_dataset))), 10*batch_size)\n",
    "\n",
    "train_dataset = train_dataset.select(idx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\n",
    "len(train_dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20480"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, return_special_tokens_mask=True)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batch_size=batch_size, batched=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a40b3db287c479eb30da24cf5b8ecaf"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "train_dataset.save_to_disk(\"prueba\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}