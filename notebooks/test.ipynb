{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Reloading"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Mirar este issue: https://github.com/huggingface/transformers/issues/10204"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Features, Value, load_from_disk\n",
    "\n",
    "train_dataset = load_from_disk(\"../data/arrow/tokenized_train\")\n",
    "test_dataset = load_from_disk(\"../data/arrow/tokenized_test\")\n",
    "\n",
    "train_dataset, test_dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'text', 'token_type_ids'],\n",
       "     num_rows: 25600000\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['attention_mask', 'input_ids', 'special_tokens_mask', 'text', 'token_type_ids'],\n",
       "     num_rows: 5215789\n",
       " }))"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from transformers import BertForMaskedLM, BertTokenizerFast\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "tokenizer.model_max_length = 128\n",
    "tokenizer.add_tokens(special_tokens)\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "tokenizer.save_pretrained(\"../data/beto-tokenizer\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('../data/beto-tokenizer/tokenizer_config.json',\n",
       " '../data/beto-tokenizer/special_tokens_map.json',\n",
       " '../data/beto-tokenizer/vocab.txt',\n",
       " '../data/beto-tokenizer/added_tokens.json',\n",
       " '../data/beto-tokenizer/tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tweets = train_dataset[:2048*10][\"text\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": tweets})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, return_special_tokens_mask=True)\n",
    "batch_size = 2048\n",
    "\n",
    "dataset = dataset.map(tokenize, batch_size=batch_size, batched=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d815d0693f564cf4927ea2a38bdcf291"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "dataset.save_to_disk(\"../data/arrow/small_dataset\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "!pwd"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/jmperez/projects/finetune_vs_scratch/notebooks\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "\n",
    "batch_size = 2048\n",
    "steps = 12_500\n",
    "\n",
    "needed_tweets = batch_size * steps\n",
    "\n",
    "needed_tweets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25600000"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import random\n",
    "\n",
    "random.seed(2021)\n",
    "\n",
    "idx = random.sample(list(range(len(train_dataset))), 10*batch_size)\n",
    "\n",
    "train_dataset = train_dataset.select(idx)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "\n",
    "len(train_dataset)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20480"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True, return_special_tokens_mask=True)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batch_size=batch_size, batched=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a40b3db287c479eb30da24cf5b8ecaf"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "train_dataset.save_to_disk(\"prueba\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}