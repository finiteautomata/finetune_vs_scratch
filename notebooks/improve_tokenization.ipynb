{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Mejorar tokenización y performance\n",
    "\n",
    "¿Qué pasa si en vez de tokenizar a 128 usamos el más largo del batch?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\"\n",
    "Run sentiment experiments\n",
    "\"\"\"\n",
    "import os\n",
    "import pathlib\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from finetune_vs_scratch.model import load_model_and_tokenizer\n",
    "from finetune_vs_scratch.preprocessing import preprocess\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset, Value, ClassLabel, Features\n",
    "from pysentimiento.tass import id2label, label2id\n",
    "from pysentimiento.metrics import compute_metrics as compute_sentiment_metrics\n",
    "\n",
    "\n",
    "project_dir = pathlib.Path(\"..\")\n",
    "data_dir = os.path.join(project_dir, \"data\")\n",
    "sentiment_dir = os.path.join(data_dir, \"sentiment\")\n",
    "\n",
    "\n",
    "def load_datasets(data_path=None, limit=None):\n",
    "    \"\"\"\n",
    "    Load sentiment datasets\n",
    "    \"\"\"\n",
    "    features = Features({\n",
    "        'text': Value('string'),\n",
    "        'lang': Value('string'),\n",
    "        'label': ClassLabel(num_classes=3, names=[\"neg\", \"neu\", \"pos\"])\n",
    "    })\n",
    "    data_path = data_path or os.path.join(sentiment_dir, \"tass.csv\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    df[\"label\"] = df[\"polarity\"].apply(lambda x: label2id[x])\n",
    "    df[\"text\"] = df[\"text\"].apply(lambda x: preprocess(x))\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(df[df[\"split\"] == \"train\"], features=features)\n",
    "    dev_dataset = Dataset.from_pandas(df[df[\"split\"] == \"dev\"], features=features)\n",
    "    test_dataset = Dataset.from_pandas(df[df[\"split\"] == \"test\"], features=features)\n",
    "\n",
    "\n",
    "    if limit:\n",
    "        \"\"\"\n",
    "        Smoke test\n",
    "        \"\"\"\n",
    "        print(\"\\n\\n\", f\"Limiting to {limit} instances\")\n",
    "        train_dataset = train_dataset.select(range(min(limit, len(train_dataset))))\n",
    "        dev_dataset = dev_dataset.select(range(min(limit, len(dev_dataset))))\n",
    "        test_dataset = test_dataset.select(range(min(limit, len(test_dataset))))\n",
    "\n",
    "\n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def run(model_name, device, data_path=None, limit=None, epochs=5, batch_size=32, eval_batch_size=32, padding=\"max_length\", **kwargs):\n",
    "    \"\"\"\n",
    "    Run sentiment analysis experiments\n",
    "    \"\"\"\n",
    "    print(\"Running sentiment experiments\")\n",
    "\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, num_labels=len(label2id), device=device)\n",
    "    train_dataset, dev_dataset, test_dataset = load_datasets(data_path=data_path, limit=limit)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], padding=padding, truncation=True)\n",
    "\n",
    "    accumulation_steps = 32 // batch_size\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
    "    dev_dataset = dev_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n",
    "\n",
    "    def format_dataset(dataset):\n",
    "        dataset = dataset.map(lambda examples: {'labels': examples['label']})\n",
    "        columns = ['input_ids', 'attention_mask', 'labels']\n",
    "        if 'token_type_ids' in dataset.features:\n",
    "            columns.append('token_type_ids')\n",
    "        dataset.set_format(type='torch', columns=columns)\n",
    "        return dataset\n",
    "\n",
    "    train_dataset = format_dataset(train_dataset)\n",
    "    dev_dataset = format_dataset(dev_dataset)\n",
    "    test_dataset = format_dataset(test_dataset)\n",
    "\n",
    "\n",
    "    output_path = tempfile.mkdtemp()\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_path,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        gradient_accumulation_steps=accumulation_steps,\n",
    "        warmup_ratio=0.1,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        do_eval=False,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=lambda x: compute_sentiment_metrics(x, id2label=id2label),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "    os.system(f\"rm -Rf {output_path}\")\n",
    "    return test_results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "%%time\n",
    "model_name = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
    "run(model_name, \"cuda\", epochs=1, padding=\"max_length\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running sentiment experiments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "52a5e59ce22d4816ab04b728f3946931"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f43fc8a145b497e92de90e83f0a8ffd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7b44af3a5e8f40169a27a799466d53f3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/4802 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f906dad8eaea4bca829a72fe76252d98"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/2443 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c16cf538c7b4258a7d04307acde3df8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/7264 [00:00<?, ?ex/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1e2e5151cad64c898045abc1809f997a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: lang, text.\n",
      "***** Running training *****\n",
      "  Num examples = 4802\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 151\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/151 01:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Neg F1</th>\n",
       "      <th>Neg Precision</th>\n",
       "      <th>Neg Recall</th>\n",
       "      <th>Neu F1</th>\n",
       "      <th>Neu Precision</th>\n",
       "      <th>Neu Recall</th>\n",
       "      <th>Pos F1</th>\n",
       "      <th>Pos Precision</th>\n",
       "      <th>Pos Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.789370</td>\n",
       "      <td>0.701587</td>\n",
       "      <td>0.706070</td>\n",
       "      <td>0.697161</td>\n",
       "      <td>0.518375</td>\n",
       "      <td>0.530343</td>\n",
       "      <td>0.506936</td>\n",
       "      <td>0.698962</td>\n",
       "      <td>0.676944</td>\n",
       "      <td>0.722461</td>\n",
       "      <td>0.642652</td>\n",
       "      <td>0.639642</td>\n",
       "      <td>0.637786</td>\n",
       "      <td>0.642186</td>\n",
       "      <td>0.642652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: lang, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2443\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /tmp/tmpffo2c7rw/checkpoint-151\n",
      "Configuration saved in /tmp/tmpffo2c7rw/checkpoint-151/config.json\n",
      "Model weights saved in /tmp/tmpffo2c7rw/checkpoint-151/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /tmp/tmpffo2c7rw/checkpoint-151 (score: 0.639641523361206).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: lang, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7264\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='227' max='227' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [227/227 00:27]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "CPU times: user 1min 25s, sys: 30.2 s, total: 1min 55s\n",
      "Wall time: 1min 50s\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7472413182258606,\n",
       " 'eval_neg_f1': 0.7282453637660485,\n",
       " 'eval_neg_precision': 0.7650805545147995,\n",
       " 'eval_neg_recall': 0.6947941476692753,\n",
       " 'eval_neu_f1': 0.5232530407822561,\n",
       " 'eval_neu_precision': 0.49259092950157163,\n",
       " 'eval_neu_recall': 0.5579857578840285,\n",
       " 'eval_pos_f1': 0.7391580283477893,\n",
       " 'eval_pos_precision': 0.7377533783783784,\n",
       " 'eval_pos_recall': 0.7405680373039424,\n",
       " 'eval_micro_f1': 0.6726321585903083,\n",
       " 'eval_macro_f1': 0.6635521650314331,\n",
       " 'eval_macro_precision': 0.6651416420936584,\n",
       " 'eval_macro_recall': 0.6644493341445923,\n",
       " 'eval_acc': 0.6726321585903083,\n",
       " 'eval_runtime': 27.6482,\n",
       " 'eval_samples_per_second': 262.729,\n",
       " 'eval_steps_per_second': 8.21,\n",
       " 'epoch': 1.0}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def run_with_collator(model_name, device, data_path=None, limit=None, epochs=5, batch_size=32, eval_batch_size=32, padding=False, **kwargs):\n",
    "    \"\"\"\n",
    "    Run sentiment analysis experiments\n",
    "    \"\"\"\n",
    "    print(\"Running sentiment experiments\")\n",
    "\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, num_labels=len(label2id), device=device)\n",
    "    train_dataset, dev_dataset, test_dataset = load_datasets(data_path=data_path, limit=limit)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch['text'], padding=padding, truncation=True)\n",
    "\n",
    "    accumulation_steps = 32 // batch_size\n",
    "\n",
    "    train_dataset = train_dataset.map(tokenize, batched=True, batch_size=batch_size)\n",
    "    dev_dataset = dev_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n",
    "    test_dataset = test_dataset.map(tokenize, batched=True, batch_size=eval_batch_size)\n",
    "\n",
    "\n",
    "    output_path = tempfile.mkdtemp()\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_path,\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=eval_batch_size,\n",
    "        gradient_accumulation_steps=accumulation_steps,\n",
    "        warmup_ratio=0.1,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        do_eval=False,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer, padding=\"longest\"),\n",
    "        compute_metrics=lambda x: compute_sentiment_metrics(x, id2label=id2label),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "    os.system(f\"rm -Rf {output_path}\")\n",
    "    return test_results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "run_with_collator(model_name, \"cuda\", epochs=1)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Running sentiment experiments\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "loading configuration file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/config.json from cache at /home/jmperez/.cache/huggingface/transformers/2416dab24674c27b5521594d6aa0929fc843a024c96711b1b5015cdff867291f.afa3630b664b4bd3e82d41660bdb96ec13236bbceadb0ae7c45c7c19f58652c7\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/pytorch_model.bin from cache at /home/jmperez/.cache/huggingface/transformers/b138da487e3aca6fae8ba8447dee4744628afa2d19b89aec47c996be858a3d1f.acf5ffb20a878065d959fdc6669d0e8869f9ee17e9c33301a68f01555159af8a\n",
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.weight', 'classifier.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/config.json from cache at /home/jmperez/.cache/huggingface/transformers/2416dab24674c27b5521594d6aa0929fc843a024c96711b1b5015cdff867291f.afa3630b664b4bd3e82d41660bdb96ec13236bbceadb0ae7c45c7c19f58652c7\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dccuchile/bert-base-spanish-wwm-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31002\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/vocab.txt from cache at /home/jmperez/.cache/huggingface/transformers/eebf656e2fb33420d0d3f12a0650df76137cfd2251e04587d7d926fba30ab1b0.bfb98b35b81356261ec63a5ff66aa147928e2c8f4d09be77fc850582a1000498\n",
      "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/tokenizer.json from cache at /home/jmperez/.cache/huggingface/transformers/85478b69412001fdb7b4cb1f5e5c5e49df292e7de8a8a27c465348fd70e817e3.1fea6aa627ed25376d8778ace0885102803fe6651fb5638d1cea57cae8ccfa7f\n",
      "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/special_tokens_map.json from cache at /home/jmperez/.cache/huggingface/transformers/78141ed1e8dcc5ff370950397ca0d1c5c9da478f54ec14544187d8a93eff1a26.f982506b52498d4adb4bd491f593dc92b2ef6be61bfdbe9d30f53f963f9f5b66\n",
      "loading file https://huggingface.co/dccuchile/bert-base-spanish-wwm-uncased/resolve/main/tokenizer_config.json from cache at /home/jmperez/.cache/huggingface/transformers/75654903071ce2eb376ae88599e5a32c926746e653c5f59fa8c72ede82bb45e5.97aaa6cf1585446e253a70715325df5cdf1791627e0480c0084d0dff6c5ebbf8\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfeb59650ab54a698027a5cebaad48e9"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86418e0cc2c34fc3a47b316ab46b483d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0/227 [00:00<?, ?ba/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "959fa43828c846b581deead4a893e1e4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: lang, text.\n",
      "***** Running training *****\n",
      "  Num examples = 4802\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 151\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='151' max='151' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [151/151 00:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Neg F1</th>\n",
       "      <th>Neg Precision</th>\n",
       "      <th>Neg Recall</th>\n",
       "      <th>Neu F1</th>\n",
       "      <th>Neu Precision</th>\n",
       "      <th>Neu Recall</th>\n",
       "      <th>Pos F1</th>\n",
       "      <th>Pos Precision</th>\n",
       "      <th>Pos Recall</th>\n",
       "      <th>Micro F1</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Macro Precision</th>\n",
       "      <th>Macro Recall</th>\n",
       "      <th>Acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.794383</td>\n",
       "      <td>0.708629</td>\n",
       "      <td>0.684985</td>\n",
       "      <td>0.733964</td>\n",
       "      <td>0.503674</td>\n",
       "      <td>0.535511</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.686399</td>\n",
       "      <td>0.676389</td>\n",
       "      <td>0.696710</td>\n",
       "      <td>0.639378</td>\n",
       "      <td>0.632901</td>\n",
       "      <td>0.632295</td>\n",
       "      <td>0.635361</td>\n",
       "      <td>0.639378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: lang, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2443\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to /tmp/tmp80o8hioa/checkpoint-151\n",
      "Configuration saved in /tmp/tmp80o8hioa/checkpoint-151/config.json\n",
      "Model weights saved in /tmp/tmp80o8hioa/checkpoint-151/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /tmp/tmp80o8hioa/checkpoint-151 (score: 0.6329007744789124).\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: lang, text.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7264\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='227' max='227' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [227/227 00:08]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7470980286598206,\n",
       " 'eval_neg_f1': 0.7365911799761621,\n",
       " 'eval_neg_precision': 0.7372188139059305,\n",
       " 'eval_neg_recall': 0.7359646138142225,\n",
       " 'eval_neu_f1': 0.5104090129806516,\n",
       " 'eval_neu_precision': 0.49220595181861126,\n",
       " 'eval_neu_recall': 0.5300101729399797,\n",
       " 'eval_pos_f1': 0.7274715660542431,\n",
       " 'eval_pos_precision': 0.7514685946678716,\n",
       " 'eval_pos_recall': 0.7049597286986011,\n",
       " 'eval_micro_f1': 0.6701541850220264,\n",
       " 'eval_macro_f1': 0.6581572890281677,\n",
       " 'eval_macro_precision': 0.6602978110313416,\n",
       " 'eval_macro_recall': 0.656978189945221,\n",
       " 'eval_acc': 0.6701541850220264,\n",
       " 'eval_runtime': 8.6203,\n",
       " 'eval_samples_per_second': 842.659,\n",
       " 'eval_steps_per_second': 26.333,\n",
       " 'epoch': 1.0}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}