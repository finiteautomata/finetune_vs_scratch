{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization\n",
    "\n",
    "### BertTweet\n",
    "\n",
    "- fastBPE\n",
    "- 64K subword\n",
    "\n",
    "### Twilbert\n",
    "- SentencePiece (fastBPE)\n",
    "- 30k subword "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from glob import glob\n",
    "\n",
    "num_files = 100\n",
    "tweet_files = glob(\"../data/filtered_tweets/*.txt\")\n",
    "\n",
    "train_files = tweet_files[:2]\n",
    "\n",
    "\n",
    "tweets = list([x.strip(\"\\n\") for x in open(\"../data/filtered_tweets/spanish-tweets-000.txt\")])[:100_000]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "len(tweets)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from tokenizers import SentencePieceUnigramTokenizer, SentencePieceBPETokenizer, BertWordPieceTokenizer\n",
    "\n",
    "tokenizer = SentencePieceBPETokenizer()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "from finetune_vs_scratch.tokenizer import tokenizer_special_tokens\n",
    "\n",
    "#tokenizer.add_special_tokens(tokenizer_special_tokens)\n",
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=30_000,\n",
    "    min_frequency=5,\n",
    "    show_progress=True,\n",
    "    special_tokens=tokenizer_special_tokens,\n",
    "    limit_alphabet=500,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "\n",
    "vocab = tokenizer.get_vocab()\n",
    "inv_vocab = {v:k for k, v in vocab.items()}\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "vocab[\"@usuario\"]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "525"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "tokenizer.encode(\"Qué hacesssss @usuario\").tokens"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['▁Qué', '▁haces', 'ss', 'ss', '▁@usuario']"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "\n",
    "\" \".join([inv_vocab[i] for i in range(1000)])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'<s> </s> <unk> <pad> <mask> ! \" # $ % & \\' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; = ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\\\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ¡ ¿ À Á È É Í Ñ Ó Ø Ú Ü à á â ã ä å ç è é ê ì í ï ñ ò ó ô õ ö ø ù ú ü ō ɜ ɪ ɴ ɾ ʀ ʏ ʔ ʕ ʝ ˎ ˶ ́ ̄ ̆ ̓ ̟ ̡ ̣ ̤ ̳ ̶ α ι ρ ω Д а е з и т א ر ل م و ي ٩ ۶ ಥ ฅ ๑ ყ ძ ხ ᄏ ᅲ ᥆ ᥉ ᥒ ᥙ ᥣ ᥱ ᥲ ᥴ ᴀ ᴅ ᴇ ᴏ ᴖ ᴗ ᴘ ᴛ ᴜ \\u200b \\u200d ⁄ \\u2066 \\u2069 ⃣ ↑ → ↓ ⇒ ∇ ∧ ≦ ≧ ⊙ ⌓ ⏱ ─ ━ ┃ ┏ ┓ ┗ ┛ ═ ║ ╗ ╚ ╭ ╮ ▁ █ □ ▪ ▶ ► ▽ ◇ ○ ◍ ☀ ★ ☆ ☑ ☟ ♪ ♫ ⚠ ✍ ✏ ✔ ❣ ➡ ⠀ ⬆ ⬇ ⬈ 、 。 「 」 『 』 【 】 〣 あ い う え お か が き く け こ ご さ し す せ そ た だ ち っ つ づ て で と ど な に の は ひ び ま み め も や よ ら り る れ ろ わ を ん ゙ ゚ ァ ア ィ イ ウ ェ オ カ キ ク グ コ サ シ ジ ス ズ タ チ ッ ツ ト ド ナ ニ ノ バ パ フ プ ベ ボ マ ミ ム メ モ ャ ュ ユ ラ リ ル レ ロ ン ・ ヾ 一 三 中 予 人 今 付 作 你 催 全 公 前 午 去 叉 可 喔 在 場 売 夜 大 好 始 定 川 巻 年 愛 描 放 新 日 是 時 月 本 森 欸 正 浪 海 無 版 特 生 用 画 発 登 看 真 神 約 紙 細 色 華 詳 誕 豪 賊 送 逆 開 零 音 ꧟ 가 거 게 고 기 나 남 년 니 다 단 대 더 들 라 랑 리 마 면 무 미 민 방 뷔 사 생 서 소 수 슈 스 시 아 야 어 에 예 요 우 은 을 이 일 자 정 제 주 준 지 짐 탄 태 하 한 해 형 홉 ︎ 🅰 🅴 🅸 🅽 🅾 🆃 🌧 🎖 🎙 🏖 🏝 🏟 🕴 🕵 🖒 🖥 🗓 🗞 🗣 ▁e ar ▁d ▁c mo io ▁l as ▁de ▁p er en us ▁a ▁s an ▁m ario usu usuario @usuario os ji ▁emo ▁@usuario ▁emoji or ta ue on ▁h ▁t es ▁q do ▁n ▁es in ▁U ▁que ▁la al ▁y RL ▁URL re un ▁v ▁en ci is te ara ▁has tag htag ▁hashtag da ▁f ▁b ▁el ▁con ra ic .. to ien ▁no ▁un ro ▁E ▁g ▁se ón ol ▁A ▁cara ando ▁me il ▁C ▁S ▁M ▁o ▁P ▁ha mp ▁por ▁r le ti ▁L de la ch ▁los ▁. ▁co ▁re ia ▁lo ▁D ▁al ▁to mos ▁per am el ▁T ▁N ir ja ▁in ora ía go ier it ▁para ion ab ás dos ma ción ▁cu ▁si ... ▁su ▁est lo ente ▁las ri ▁del ▁te ▁qu jo ▁mi ec ▁Y gu ac ▁una ▁I dad ▁ll ▁B !! isa ▁H ▁mu ad res ▁llor ba ▁le cu cia ▁risa qu ca ▁J vi tr mb ▁Es ▁man ▁llorando ▁Q oy ▁an ▁i ica ent ▁son iendo ▁R ▁des que ▁V co ▁\" ▁G ▁di ▁th ▁cora AJ ga ▁más ▁coraz ▁1 ▁w me ▁F era ing di ur ▁so ▁vi ▁j ico ul ues ter ▁ch ▁pro tra ▁pa ada ez tes ▁yo ▁2 ▁No ▁mo ▁ten ero ce ▁ver tos ▁como ran ▁tien ▁ex ▁esta ▁ser ▁pero ▁corazón tas ie iz én ▁O ▁as ▁po ▁ya ▁ap án tu ▁ar ay AJAJ ▁par at ios son mi av lar enta olo ▁tu ud tar ▁El ▁ro ▁person jos ▁pi ver ono cion ▁La ▁ca ana ▁sal pa vo ▁ac ve ▁tra ▁the ten ▁( ▁fu ▁sonr ▁pue ▁pas idad no se ina tor der jaja per ▁¿ lan OS ▁ta uen cer ▁tiene ▁todo va ER ▁ni cha ▁mas ales ▁mis so ▁hab ▁k ido ▁sin cho ▁pe !!! das ▁mano tro dio ▁mar ▁quier gar ▁Que ▁res ▁comp ig ño ▁cas za ens ▁ojos 00 ▁Me ita ▁ab EN ▁tan ▁En ▁ma id ▁hay pe men ▁va ice ▁esto ▁sonriendo tan ib acion ▁ah ▁pre man ON ños min AN ín ▁sab ob ▁and ión cias ▁cre ▁porque ones ▁nos AS ▁este eces las des mas los ▁está ut ado ▁can ▁piel ?? amil ▁tono je ES ▁Si ▁sus ito ▁Yo ▁, olvi ores ▁he ▁cuando ▁- ▁com ▁Se ru AR ▁lle po ▁os és ame ▁W ▁todos mente ici ▁bien du ▁cor den ▁of OR et ▁rev ▁jo laro ▁persona UE ▁ra ís les ▁famil 19 iste ▁3 ▁Co ▁hacer ▁eso ▁mej mil ▁día ▁fav zo ena ▁pu ▁imp ▁ent ▁fr gun ún oo ▁for xico iento ▁fuer ▁jaja ▁buen ▁ho ▁mal bre ▁DE ▁favor ▁claro ▁qué ▁pres emos ién mar ▁cos ista don ▁Un mpre ▁bon ▁vida ▁ven ▁nada ▁vo ▁dis ías ▁ti ▁hoy ▁fue dose ▁traba ▁men ▁ne ▁muy ▁ES ▁Por ed ▁les jer ▁pr gan ▁De ▁Con ▁car ill ▁gran ces ▁amo ▁cal ble ▁deb ▁mejor ▁Lo o,'"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "tokenizer_path = \"./sentence-piece-tokenizer\"\n",
    "!mkdir $tokenizer_path\n",
    "vocab_file, merges_file = tokenizer.save_model(tokenizer_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘./sentence-piece-tokenizer’: File exists\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "from finetune_vs_scratch.tokenizer import MyTokenizer\n",
    "\n",
    "t_tokenizer = MyTokenizer(\n",
    "    vocab_file,\n",
    "    merges_file,\n",
    ")\n",
    "\n",
    "#sorted(vars(t_tokenizer).keys())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['_additional_special_tokens',\n",
       " '_bos_token',\n",
       " '_cls_token',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_eos_token',\n",
       " '_mask_token',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_sep_token',\n",
       " '_unk_token',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'bpe_ranks',\n",
       " 'cache',\n",
       " 'decoder',\n",
       " 'deprecation_warnings',\n",
       " 'encoder',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'lower',\n",
       " 'merges_file',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'normalization',\n",
       " 'padding_side',\n",
       " 'special_puncts',\n",
       " 'unique_no_split_tokens',\n",
       " 'verbose',\n",
       " 'vocab_file']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "t_tokenizer.save_pretrained(\"./mytokenizer\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./mytokenizer/tokenizer_config.json',\n",
       " './mytokenizer/special_tokens_map.json',\n",
       " './mytokenizer/vocab.txt',\n",
       " './mytokenizer/bpe.codes',\n",
       " './mytokenizer/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "MyTokenizer.from_pretrained(\"./mytokenizer\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'normalization': True, 'bos_token': '<s>', 'eos_token': '</s>', 'sep_token': '</s>', 'cls_token': '<s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'lower': False, 'special_tokens_map_file': './mytokenizer/special_tokens_map.json', 'tokenizer_file': None, 'name_or_path': './mytokenizer'}\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'vocab_file' and 'merges_file'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_365518/2240273569.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMyTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./mytokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1717\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1718\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1719\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m         )\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             raise OSError(\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'vocab_file' and 'merges_file'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Qué garcha pasa?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tugo = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "() {'model_max_length': 128, 'vocab_file': '/home/jmperez/.cache/huggingface/transformers/9a877d0d57efbfeae96fec396a35595dc8c4685fe2b7b2049c6c094e24a0e8bf.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982', 'merges_file': '/home/jmperez/.cache/huggingface/transformers/1c2d05a06ac61a063ad62a7590731a28cc62f58e2802c76b5f993165f25894a9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4', 'special_tokens_map_file': None, 'tokenizer_file': None, 'name_or_path': 'vinai/bertweet-base'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "tugo.vocab_file"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/home/jmperez/.cache/huggingface/transformers/9a877d0d57efbfeae96fec396a35595dc8c4685fe2b7b2049c6c094e24a0e8bf.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "\n",
    "tugo.save_pretrained(\"./pepe\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('./pepe/tokenizer_config.json',\n",
       " './pepe/special_tokens_map.json',\n",
       " './pepe/vocab.txt',\n",
       " './pepe/bpe.codes',\n",
       " './pepe/added_tokens.json')"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## WordPiece"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=True,\n",
    "    strip_accents=False,\n",
    "    lowercase=False,\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from finetune_vs_scratch.preprocessing import special_tokens\n",
    "\n",
    "tokenizer.add_special_tokens(special_tokens)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "tokenizer.train_from_iterator(\n",
    "    tweets,\n",
    "    vocab_size=40_000,\n",
    "    min_frequency=2,\n",
    "    show_progress=True,\n",
    "\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"] + special_tokens,\n",
    "    limit_alphabet=600,\n",
    "    wordpieces_prefix=\"##\",\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!mkdir test_tokenizer\n",
    "tokenizer.save_model(\"./test_tokenizer/\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "mkdir: cannot create directory ‘test_tokenizer’: File exists\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['./test_tokenizer/vocab.txt']"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reload"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from transformers import BertTokenizer, BertTokenizerFast\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"./test_tokenizer/\",\n",
    "    never_split=special_tokens,\n",
    ")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading trained tokenizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\n",
    "!pip freeze | grep transf"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: Could not generate requirement for distribution -orch 1.9.0+cu111 (/home/jmperez/.cache/pypoetry/virtualenvs/finetune-vs-scratch-gHiQbun3-py3.8/lib/python3.8/site-packages): Parse error at \"'-orch==1'\": Expected W:(abcd...)\u001b[0m\n",
      "transformers @ file:///home/jmperez/.cache/pypoetry/artifacts/d7/d1/6f/c08a86d09ebb99ef7233126f12dce131f0bcf38f23b1c8aa8d2065c528/transformers-4.8.2-py3-none-any.whl\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer, BertTokenizer\n",
    "from finetune_vs_scratch.model import load_tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\n",
    "    \"../models/tokenizers/betito_cased_accents/\",\n",
    "    never_split=[\"@usuario\"]\n",
    ")\n",
    "\n",
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Si es Fast, le tenemos que poner `additional_special_tokens`, LTA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "from transformers import BertTokenizerFast, AutoTokenizer, BertTokenizer\n",
    "from finetune_vs_scratch.model import load_tokenizer\n",
    "\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained(\n",
    "    \"../models/tokenizers/betito_cased_accents/\",\n",
    "    additional_special_tokens=[\"@usuario\"]\n",
    ")\n",
    "\n",
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5, 3], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "bert_tokenizer(\"@usuario\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_ids': [2, 38, 1164, 3], 'token_type_ids': [0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import re \n",
    "\n",
    "tweet = \"@usuario @usuario jajaja gil emoji riendo emoji emoji riendo emoji\"\n",
    "\n",
    "tweet = re.sub(\"emoji.*?emoji\", \"emoji\", tweet)\n",
    "print(tweet)\n",
    "tokens = bert_tokenizer(tweet)[\"input_ids\"]\n",
    "print(tokens)\n",
    "print(bert_tokenizer.decode(tokens))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "@usuario @usuario jajaja gil emoji emoji\n",
      "[2, 38, 1164, 38, 1164, 5733, 621, 5411, 8, 8, 3]\n",
      "[CLS] @ usuario @ usuario jajaja gil emoji emoji [SEP]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=False, do_lower_case=True, do_basic_tokenize=True)\n",
    "tokenizer.add_tokens(['graft', 'grafts'])"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4bd9500643240529c87f4d39be57585"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e43bf239ef5145f1a3a49fba534210bc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a902248c7b547af94768d8d17bb785e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "tokenizer.convert_ids_to_tokens(tokenizer.encode(\"graft\"))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['[CLS]', 'graft', '[SEP]']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('finetune-vs-scratch-gHiQbun3-py3.8': poetry)"
  },
  "interpreter": {
   "hash": "28c1932dff7617228923490e32f133f79d588eb74ca6c2b1f196ab0fdc858ed2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}